{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "from typing import Tuple, Dict, Any, List\n",
    "import panel as pn\n",
    "from matplotlib.figure import Figure\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilenames\n",
    "from tkinter import Tk, Toplevel, Button, Checkbutton, IntVar, Label, Frame, filedialog\n",
    "import cupy as cp\n",
    "import easygui\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from joblib import Parallel, delayed\\\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected GeoJSON files: ['C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/geology_clipped.geojson', 'C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/possible_anomalies_clipped.geojson']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to interactively select files\n",
    "def select_geojson_files():\n",
    "    # Create a Tkinter root window (hidden)\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    root.attributes(\"-topmost\", True)  # Bring the dialog to the front\n",
    "\n",
    "    # Open the file selection dialog\n",
    "    geojson_files = filedialog.askopenfilenames(\n",
    "        title=\"Select GeoJSON Files\",\n",
    "        filetypes=[(\"GeoJSON files\", \"*.geojson\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "    \n",
    "    root.destroy()  # Close the root window after selection\n",
    "    return list(geojson_files)  # Convert tuple to list and return\n",
    "\n",
    "# Use the function to select files\n",
    "geojson_files = select_geojson_files()\n",
    "\n",
    "# Print the selected files for verification\n",
    "print(f\"Selected GeoJSON files: {geojson_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56950cf47c54d399c76a10137850075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Select columns for geology_clipped.geojson:', options=('fid', 'CD_CORRELA', 'ESCAL…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938245a317e24b97be62b888f5048af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit Selection', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94d5898aac446029d2179bd6f12ddbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Select columns for possible_anomalies_clipped.geojson:', options=('fid', 'id', 'ge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee1e0ae79c0424f84ecd025c04fccfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit Selection', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns from C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/geology_clipped.geojson: [('C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/geology_clipped.geojson', 'SUBTIPO_DE'), ('C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/geology_clipped.geojson', 'GEOCHRON_A'), ('C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/geology_clipped.geojson', 'UNIDAD_GEN')]\n",
      "Selected columns from C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/possible_anomalies_clipped.geojson: [('C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/possible_anomalies_clipped.geojson', 'fid')]\n"
     ]
    }
   ],
   "source": [
    "# Store the selected columns (fields) in the correct format\n",
    "features_to_process = []\n",
    "\n",
    "def select_columns(geojson_file):\n",
    "    \"\"\"Function to display column selection widgets for a given GeoJSON file.\"\"\"\n",
    "    # Read the GeoJSON file using GeoPandas\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    \n",
    "    # Get the list of columns\n",
    "    columns = gdf.columns.tolist()\n",
    "    \n",
    "    # Create a multiple selection widget for columns\n",
    "    selection = widgets.SelectMultiple(\n",
    "        options=columns,\n",
    "        description=f'Select columns for {os.path.basename(geojson_file)}:',\n",
    "        rows=10\n",
    "    )\n",
    "    \n",
    "    # Display the widget and button\n",
    "    display(selection)\n",
    "\n",
    "    # Define button click event\n",
    "    def on_button_click(b):\n",
    "        # For each selected column, create a tuple of (geojson_file, column_name)\n",
    "        selected_columns = [(geojson_file, col) for col in selection.value]\n",
    "        features_to_process.extend(selected_columns)\n",
    "        print(f'Selected columns from {geojson_file}: {selected_columns}')\n",
    "    \n",
    "    # Create and display button\n",
    "    button = widgets.Button(description=\"Submit Selection\")\n",
    "    button.on_click(on_button_click)\n",
    "    display(button)\n",
    "\n",
    "# Iterate through each GeoJSON file and let the user select columns\n",
    "for file in geojson_files:\n",
    "    select_columns(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/geology_clipped.geojson', 'SUBTIPO_DE'), ('C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/geology_clipped.geojson', 'GEOCHRON_A'), ('C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/geology_clipped.geojson', 'UNIDAD_GEN'), ('C:/Users/TyHow/Documents/3. Work/GIS Stuff/ML_pilot_data/possible_anomalies_clipped.geojson', 'fid')]\n"
     ]
    }
   ],
   "source": [
    "print(features_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION DEFINITION - CPU (only works for geol layer)\n",
    "\n",
    "def geojson_to_numpy_grid_3d_batch(\n",
    "    grid_size: Tuple[int, int],  # Grid size for the output array\n",
    "    target_crs: str = \"EPSG:3857\"  # Web Mercator projection\n",
    ") -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, Dict[Any, int]], List[Dict[str, Any]]]:\n",
    "    # Open a file selection dialog for the user to select multiple GeoJSON files\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    root.attributes(\"-topmost\", True)  # Bring the dialog to the front\n",
    "\n",
    "    # Open the file selection dialog\n",
    "    geojson_files = askopenfilenames(\n",
    "        title=\"Select GeoJSON Files\",\n",
    "        filetypes=[(\"GeoJSON files\", \"*.geojson\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "\n",
    "    all_feature_grids = {}\n",
    "    all_feature_mappings = {}\n",
    "    geospatial_info_list = []\n",
    "\n",
    "    for geojson_file in geojson_files:\n",
    "        # Read the GeoJSON file\n",
    "        gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "        # Reproject to target CRS\n",
    "        gdf = gdf.to_crs(target_crs)\n",
    "\n",
    "        # Get the total bounds of all geometries\n",
    "        minx, miny, maxx, maxy = gdf.total_bounds\n",
    "\n",
    "        # Create a fixed-size grid\n",
    "        x = np.linspace(minx, maxx, grid_size[1] + 1)\n",
    "        y = np.linspace(miny, maxy, grid_size[0] + 1)\n",
    "\n",
    "        # Automatically extract all relevant feature columns, excluding geometry columns\n",
    "        feature_columns = [col for col in gdf.columns if col != gdf.geometry.name]\n",
    "\n",
    "        # Get the filename without extension for prefixing\n",
    "        filename_prefix = os.path.splitext(os.path.basename(geojson_file))[0]\n",
    "\n",
    "        # Dictionary to hold grids and category mappings for each feature column\n",
    "        feature_grids = {}\n",
    "        feature_mappings = {}\n",
    "\n",
    "        # Store geospatial information for each file\n",
    "        geospatial_info = {\n",
    "            'transform': (minx, miny, maxx, maxy),\n",
    "            'crs': target_crs,\n",
    "            'file_name': filename_prefix\n",
    "        }\n",
    "        geospatial_info_list.append(geospatial_info)\n",
    "\n",
    "        # Iterate over each feature column\n",
    "        for feature_column in feature_columns:\n",
    "            # Get unique categories and create a mapping to integers\n",
    "            unique_categories = gdf[feature_column].unique()\n",
    "            \n",
    "            # Prefix each feature class with the filename\n",
    "            category_to_int = {f\"{filename_prefix}_{cat}\": i for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "            # Initialize the 2D NumPy array with -1 (representing no data)\n",
    "            grid = np.full(grid_size, -1, dtype=int)\n",
    "\n",
    "            # Create a spatial index for faster intersection checks\n",
    "            sindex = gdf.sindex\n",
    "\n",
    "            # Pre-compute cell geometries\n",
    "            cells = [box(x[j], y[i], x[j + 1], y[i + 1])\n",
    "                     for i in range(grid_size[0])\n",
    "                     for j in range(grid_size[1])]\n",
    "\n",
    "            # Vectorized operations for intersection\n",
    "            def process_cell(cell, possible_matches):\n",
    "                if possible_matches.empty:\n",
    "                    return -1\n",
    "                intersections = possible_matches.geometry.intersection(cell)\n",
    "                intersection_areas = intersections.area\n",
    "                largest_intersection_idx = intersection_areas.idxmax()\n",
    "                category = possible_matches.loc[largest_intersection_idx, feature_column]\n",
    "                return category_to_int[f\"{filename_prefix}_{category}\"]\n",
    "\n",
    "            # Iterate through each cell in the grid\n",
    "            for idx, cell in enumerate(cells):\n",
    "                i, j = divmod(idx, grid_size[1])\n",
    "\n",
    "                # Use the spatial index to find potential intersecting polygons\n",
    "                possible_matches_index = list(sindex.intersection(cell.bounds))\n",
    "                if not possible_matches_index:\n",
    "                    continue\n",
    "\n",
    "                # Check for actual intersection and assign the feature value\n",
    "                possible_matches = gdf.iloc[possible_matches_index]\n",
    "                grid[i, j] = process_cell(cell, possible_matches)\n",
    "\n",
    "            # Store the grid and category mapping for this feature\n",
    "            feature_grids[f\"{filename_prefix}_{feature_column}\"] = grid\n",
    "            feature_mappings[f\"{filename_prefix}_{feature_column}\"] = category_to_int\n",
    "\n",
    "        # Merge with all feature grids and mappings\n",
    "        all_feature_grids.update(feature_grids)\n",
    "        all_feature_mappings.update(feature_mappings)\n",
    "\n",
    "    # Stack all grids into a 3D array\n",
    "    grid_3d = np.stack(list(all_feature_grids.values()), axis=0)\n",
    "\n",
    "    return grid_3d, all_feature_grids, all_feature_mappings, geospatial_info_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARALLEL PROCESSING FUNCTION - CPU - HYBRID STRATEGY\n",
    "\n",
    "def process_cell(idx, cell, gdf, sindex, feature_column, category_to_int, filename_prefix):\n",
    "    \"\"\"\n",
    "    Process a single cell by finding intersections and determining the feature value.\n",
    "    \"\"\"\n",
    "    i, j = divmod(idx, grid_size[1])\n",
    "\n",
    "    # Use the spatial index to find potential intersecting polygons\n",
    "    possible_matches_index = list(sindex.intersection(cell.bounds))\n",
    "    if not possible_matches_index:\n",
    "        return i, j, np.nan  # No intersecting features, return NaN for no data\n",
    "\n",
    "    # Check for actual intersection and assign the feature value\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    if possible_matches.empty:\n",
    "        return i, j, np.nan\n",
    "\n",
    "    # Calculate intersections more precisely\n",
    "    intersections = possible_matches.geometry.intersection(cell)\n",
    "\n",
    "    # Consider all non-zero intersections\n",
    "    valid_intersections = intersections[intersections.area > 0]\n",
    "\n",
    "    if valid_intersections.empty:\n",
    "        return i, j, np.nan\n",
    "\n",
    "    # Determine strategy based on the number of intersecting features\n",
    "    if len(valid_intersections) > 5:  # Threshold for choosing strategy\n",
    "        # Many small polygons: Sum the areas for each unique category\n",
    "        areas_per_category = {}\n",
    "        for idx, intersection in enumerate(valid_intersections):\n",
    "            if not intersection.is_empty:\n",
    "                category = possible_matches.iloc[idx][feature_column]\n",
    "                category_key = f\"{filename_prefix}_{category}\"\n",
    "                if category_key not in areas_per_category:\n",
    "                    areas_per_category[category_key] = 0\n",
    "                areas_per_category[category_key] += intersection.area\n",
    "\n",
    "        # Choose the category with the largest cumulative area\n",
    "        if areas_per_category:\n",
    "            max_category = max(areas_per_category, key=areas_per_category.get)\n",
    "            return i, j, category_to_int[max_category]\n",
    "        else:\n",
    "            return i, j, np.nan  # Fallback to NaN\n",
    "\n",
    "    else:\n",
    "        # Few large polygons: Choose the largest single intersection by area\n",
    "        largest_intersection_idx = valid_intersections.area.idxmax()\n",
    "        category = possible_matches.loc[largest_intersection_idx, feature_column]\n",
    "        return i, j, category_to_int[f\"{filename_prefix}_{category}\"]\n",
    "\n",
    "def process_feature_column(geojson_file, feature_column, grid_size, target_crs, filename_prefix, x, y):\n",
    "    # Read the GeoJSON file\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "    # Reproject to target CRS\n",
    "    gdf = gdf.to_crs(target_crs)\n",
    "\n",
    "    # Get unique categories and create a mapping to integers\n",
    "    unique_categories = gdf[feature_column].unique()\n",
    "    category_to_int = {f\"{filename_prefix}_{cat}\": i for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "    # Initialize the 2D NumPy array with NaN (representing no data)\n",
    "    grid = np.full(grid_size, np.nan)\n",
    "\n",
    "    # Create a spatial index for faster intersection checks\n",
    "    sindex = gdf.sindex\n",
    "\n",
    "    # Pre-compute cell geometries\n",
    "    cells = [box(x[j], y[i], x[j + 1], y[i + 1])\n",
    "             for i in range(grid_size[0])\n",
    "             for j in range(grid_size[1])]\n",
    "\n",
    "    # Use joblib to parallelize cell processing\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_cell)(\n",
    "        idx, cell, gdf, sindex, feature_column, category_to_int, filename_prefix\n",
    "    ) for idx, cell in enumerate(cells))\n",
    "\n",
    "    # Fill the grid with the results\n",
    "    for i, j, value in results:\n",
    "        grid[i, j] = value\n",
    "\n",
    "    # Return the grid and category mapping for this feature\n",
    "    return (f\"{filename_prefix}_{feature_column}\", grid, category_to_int)\n",
    "\n",
    "def geojson_to_numpy_grid_3d_batch(\n",
    "    grid_size: Tuple[int, int],  # Grid size for the output array\n",
    "    target_crs: str = \"EPSG:3857\"  # Web Mercator projection\n",
    ") -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, Dict[Any, int]], List[Dict[str, Any]]]:\n",
    "    # Open a file selection dialog for the user to select multiple GeoJSON files\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    root.attributes(\"-topmost\", True)  # Bring the dialog to the front\n",
    "\n",
    "    # Open the file selection dialog\n",
    "    geojson_files = askopenfilenames(\n",
    "        title=\"Select GeoJSON Files\",\n",
    "        filetypes=[(\"GeoJSON files\", \"*.geojson\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "\n",
    "    all_feature_grids = {}\n",
    "    all_feature_mappings = {}\n",
    "    geospatial_info_list = []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for geojson_file in geojson_files:\n",
    "        # Read the GeoJSON file to get the total bounds\n",
    "        gdf = gpd.read_file(geojson_file)\n",
    "        gdf = gdf.to_crs(target_crs)\n",
    "        minx, miny, maxx, maxy = gdf.total_bounds\n",
    "\n",
    "        # Create a fixed-size grid\n",
    "        x = np.linspace(minx, maxx, grid_size[1] + 1)\n",
    "        y = np.linspace(miny, maxy, grid_size[0] + 1)\n",
    "\n",
    "        # Automatically extract all relevant feature columns, excluding geometry columns\n",
    "        feature_columns = [col for col in gdf.columns if col != gdf.geometry.name]\n",
    "\n",
    "        # Get the filename without extension for prefixing\n",
    "        filename_prefix = os.path.splitext(os.path.basename(geojson_file))[0]\n",
    "\n",
    "        # Store geospatial information for each file\n",
    "        geospatial_info = {\n",
    "            'transform': (minx, miny, maxx, maxy),\n",
    "            'crs': target_crs,\n",
    "            'file_name': filename_prefix\n",
    "        }\n",
    "        geospatial_info_list.append(geospatial_info)\n",
    "\n",
    "        # Use joblib to parallelize the processing of each feature column\n",
    "        results.extend(Parallel(n_jobs=-1)(delayed(process_feature_column)(\n",
    "            geojson_file, feature_column, grid_size, target_crs, filename_prefix, x, y\n",
    "        ) for feature_column in feature_columns))\n",
    "\n",
    "    # Process results to merge grids and mappings\n",
    "    for feature_name, grid, category_to_int in results:\n",
    "        all_feature_grids[feature_name] = grid\n",
    "        all_feature_mappings[feature_name] = category_to_int\n",
    "\n",
    "    # Stack all grids into a 3D array\n",
    "    grid_3d = np.stack(list(all_feature_grids.values()), axis=0)\n",
    "\n",
    "    return grid_3d, all_feature_grids, all_feature_mappings, geospatial_info_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARALLEL PROCESSING FUNCTION - CPU - HYBRID STRATEGY - MANUAL INPUTS\n",
    "\n",
    "# Function to process each cell in the grid\n",
    "def process_cell(idx, cell, gdf, sindex, feature_column, category_to_int, filename_prefix):\n",
    "    i, j = divmod(idx, grid_size[1])\n",
    "\n",
    "    # Use the spatial index to find potential intersecting polygons\n",
    "    possible_matches_index = list(sindex.intersection(cell.bounds))\n",
    "    if not possible_matches_index:\n",
    "        return i, j, np.nan\n",
    "\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    if possible_matches.empty:\n",
    "        return i, j, np.nan\n",
    "\n",
    "    intersections = possible_matches.geometry.intersection(cell)\n",
    "    valid_intersections = intersections[intersections.area > 0]\n",
    "\n",
    "    if valid_intersections.empty:\n",
    "        return i, j, np.nan\n",
    "\n",
    "    if len(valid_intersections) > 5:\n",
    "        areas_per_category = {}\n",
    "        for idx, intersection in enumerate(valid_intersections):\n",
    "            if not intersection.is_empty:\n",
    "                category = possible_matches.iloc[idx][feature_column]\n",
    "                category_key = f\"{filename_prefix}_{category}\"\n",
    "                if category_key not in areas_per_category:\n",
    "                    areas_per_category[category_key] = 0\n",
    "                areas_per_category[category_key] += intersection.area\n",
    "\n",
    "        if areas_per_category:\n",
    "            max_category = max(areas_per_category, key=areas_per_category.get)\n",
    "            return i, j, category_to_int[max_category]\n",
    "        else:\n",
    "            return i, j, np.nan\n",
    "    else:\n",
    "        largest_intersection_idx = valid_intersections.area.idxmax()\n",
    "        category = possible_matches.loc[largest_intersection_idx, feature_column]\n",
    "        return i, j, category_to_int[f\"{filename_prefix}_{category}\"]\n",
    "\n",
    "# Function to process each feature column\n",
    "def process_feature_column(geojson_file, feature_column, grid_size, target_crs, filename_prefix, x, y):\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    print(f\"Processing feature column: {feature_column} from file: {geojson_file}\")\n",
    "\n",
    "    gdf = gdf.to_crs(target_crs)\n",
    "    if gdf.empty:\n",
    "        print(f\"GeoDataFrame for {geojson_file} is empty after reprojecting. Skipping column: {feature_column}\")\n",
    "        return None\n",
    "\n",
    "    unique_categories = gdf[feature_column].unique()\n",
    "    print(f\"Unique categories in {feature_column}: {unique_categories}\")\n",
    "    category_to_int = {f\"{filename_prefix}_{cat}\": i for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "    grid = np.full(grid_size, np.nan)\n",
    "    sindex = gdf.sindex\n",
    "\n",
    "    cells = [box(x[j], y[i], x[j + 1], y[i + 1])\n",
    "             for i in range(grid_size[0])\n",
    "             for j in range(grid_size[1])]\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_cell)(\n",
    "        idx, cell, gdf, sindex, feature_column, category_to_int, filename_prefix\n",
    "    ) for idx, cell in enumerate(cells))\n",
    "\n",
    "    if not results:\n",
    "        print(f\"No results were generated for feature column: {feature_column} from file: {geojson_file}\")\n",
    "        return None\n",
    "\n",
    "    for i, j, value in results:\n",
    "        grid[i, j] = value\n",
    "\n",
    "    return (f\"{filename_prefix}_{feature_column}\", grid, category_to_int)\n",
    "\n",
    "# Batch processing function\n",
    "def geojson_to_numpy_grid_3d_batch(\n",
    "    grid_size: Tuple[int, int],  # Grid size for the output array\n",
    "    geojson_files: List[str],  # List of GeoJSON files\n",
    "    features_to_process: List[Tuple[str, str]],  # List of (file, feature) tuples to process\n",
    "    target_crs: str = \"EPSG:3857\"  # Web Mercator projection\n",
    ") -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, Dict[Any, int]], List[Dict[str, Any]]]:\n",
    "    all_feature_grids = {}\n",
    "    all_feature_mappings = {}\n",
    "    geospatial_info_list = []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Process each file and its corresponding features\n",
    "    for geojson_file in geojson_files:\n",
    "        # Get the filename without extension for prefixing\n",
    "        filename_prefix = os.path.splitext(os.path.basename(geojson_file))[0]\n",
    "\n",
    "        # Read the GeoJSON file to get the total bounds\n",
    "        gdf = gpd.read_file(geojson_file)\n",
    "        gdf = gdf.to_crs(target_crs)\n",
    "        minx, miny, maxx, maxy = gdf.total_bounds\n",
    "\n",
    "        x = np.linspace(minx, maxx, grid_size[1] + 1)\n",
    "        y = np.linspace(miny, maxy, grid_size[0] + 1)\n",
    "\n",
    "        # Extract relevant features for this file\n",
    "        file_features = [feature for file, feature in features_to_process if file == geojson_file]\n",
    "\n",
    "        # Store geospatial information for each file\n",
    "        geospatial_info = {\n",
    "            'transform': (minx, miny, maxx, maxy),\n",
    "            'crs': target_crs,\n",
    "            'file_name': filename_prefix\n",
    "        }\n",
    "        geospatial_info_list.append(geospatial_info)\n",
    "\n",
    "        # Use joblib to parallelize the processing of each feature column\n",
    "        results.extend(Parallel(n_jobs=-1)(delayed(process_feature_column)(\n",
    "            geojson_file, feature_column, grid_size, target_crs, filename_prefix, x, y\n",
    "        ) for feature_column in file_features))\n",
    "\n",
    "    for feature_name, grid, category_to_int in results:\n",
    "        all_feature_grids[feature_name] = grid\n",
    "        all_feature_mappings[feature_name] = category_to_int\n",
    "\n",
    "    grid_3d = np.stack(list(all_feature_grids.values()), axis=0)\n",
    "\n",
    "    return grid_3d, all_feature_grids, all_feature_mappings, geospatial_info_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated grid size: (25, 20)\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE GRID SIZE\n",
    "\n",
    "def compute_grid_size(geojson_file: str, short_edge_cells: int = 1200) -> Tuple[int, int]:\n",
    "    # Read the GeoJSON file\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    \n",
    "    # Get the bounding box of the masking region\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    \n",
    "    # Calculate width and height of the bounding box\n",
    "    width = maxx - minx\n",
    "    height = maxy - miny\n",
    "\n",
    "    # Determine which is the short and long edge\n",
    "    if width < height:\n",
    "        short_edge = width\n",
    "        long_edge = height\n",
    "        orientation = 'portrait'\n",
    "    else:\n",
    "        short_edge = height\n",
    "        long_edge = width\n",
    "        orientation = 'landscape'\n",
    "\n",
    "    # Compute the aspect ratio\n",
    "    aspect_ratio = long_edge / short_edge\n",
    "\n",
    "    # Compute the number of cells for the long edge\n",
    "    long_edge_cells = int(short_edge_cells * aspect_ratio)\n",
    "\n",
    "    # Determine the grid size based on the orientation\n",
    "    if orientation == 'portrait':\n",
    "        grid_size = (short_edge_cells, long_edge_cells)\n",
    "    else:\n",
    "        grid_size = (long_edge_cells, short_edge_cells)\n",
    "\n",
    "    return grid_size\n",
    "\n",
    "mask_file = r\"C:\\Users\\TyHow\\Documents\\3. Work\\GIS Stuff\\ML_pilot_data\\MASK.geojson\"\n",
    "grid_size = compute_grid_size(mask_file, short_edge_cells=20)[::-1]\n",
    "print(f\"Calculated grid size: {grid_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the 3D grid array: (4, 25, 20)\n",
      "Feature grids: dict_keys(['geology_clipped_SUBTIPO_DE', 'geology_clipped_GEOCHRON_A', 'geology_clipped_UNIDAD_GEN', 'possible_anomalies_clipped_fid'])\n",
      "Feature mappings: {'geology_clipped_SUBTIPO_DE': {'geology_clipped_AMBIENTE SEDIMENTARIO': 0, 'geology_clipped_AMBIENTE PLUTONICO': 1, 'geology_clipped_AMBIENTE SUBVOLCANICO HIPABISAL': 2, 'geology_clipped_AMBIENTE VOLCANICO': 3, 'geology_clipped_AMBIENTE VOLCANICO SEDIMENTARIO': 4}, 'geology_clipped_GEOCHRON_A': {'geology_clipped_None': 0, 'geology_clipped_66': 1, 'geology_clipped_55,3': 2, 'geology_clipped_79': 3}, 'geology_clipped_UNIDAD_GEN': {'geology_clipped_Kp': 0, 'geology_clipped_Hf': 1, 'geology_clipped_Krt': 2, 'geology_clipped_PlHa': 3, 'geology_clipped_Jm': 4, 'geology_clipped_PlHc': 5, 'geology_clipped_Plf': 6, 'geology_clipped_PaEdc': 7, 'geology_clipped_KPadp(gd)': 8, 'geology_clipped_PaEdh(gd)': 9, 'geology_clipped_PaEdc(gd)': 10, 'geology_clipped_KPadp(g)': 11, 'geology_clipped_Pg': 12, 'geology_clipped_Cg': 13, 'geology_clipped_PaEgdr': 14, 'geology_clipped_PaEgdg': 15, 'geology_clipped_Edm': 16, 'geology_clipped_PaEgdg(d)': 17, 'geology_clipped_Eih(a)': 18, 'geology_clipped_Ksih(a)': 19, 'geology_clipped_Kv(b)': 20, 'geology_clipped_Kle': 21, 'geology_clipped_Ja': 22, 'geology_clipped_Kv(a)': 23, 'geology_clipped_Jtc(b)': 24, 'geology_clipped_Jtc(a)': 25, 'geology_clipped_Jtc(c)': 26}, 'possible_anomalies_clipped_fid': {'possible_anomalies_clipped_1': 0, 'possible_anomalies_clipped_2': 1, 'possible_anomalies_clipped_3': 2, 'possible_anomalies_clipped_4': 3, 'possible_anomalies_clipped_5': 4, 'possible_anomalies_clipped_6': 5, 'possible_anomalies_clipped_7': 6, 'possible_anomalies_clipped_8': 7, 'possible_anomalies_clipped_9': 8, 'possible_anomalies_clipped_10': 9, 'possible_anomalies_clipped_11': 10, 'possible_anomalies_clipped_12': 11, 'possible_anomalies_clipped_13': 12, 'possible_anomalies_clipped_14': 13, 'possible_anomalies_clipped_15': 14, 'possible_anomalies_clipped_16': 15, 'possible_anomalies_clipped_17': 16, 'possible_anomalies_clipped_18': 17, 'possible_anomalies_clipped_19': 18, 'possible_anomalies_clipped_20': 19, 'possible_anomalies_clipped_21': 20, 'possible_anomalies_clipped_22': 21}}\n",
      "Geospatial information for each file: [{'transform': (-7886233.879223923, -3632432.80288566, -7848762.490354943, -3583369.988284047), 'crs': 'EPSG:3857', 'file_name': 'geology_clipped'}, {'transform': (-7886233.879223923, -3632418.5919462033, -7851197.861443554, -3583384.025986247), 'crs': 'EPSG:3857', 'file_name': 'possible_anomalies_clipped'}]\n"
     ]
    }
   ],
   "source": [
    "#RUN FUNCTION\n",
    "\n",
    "#grid_size = (1200, 1550)  # Define the grid size\n",
    "\n",
    "# Call the function\n",
    "#grid_3d, feature_grids, feature_mappings, geospatial_info_list = geojson_to_numpy_grid_3d_batch(grid_size)\n",
    "grid_3d, feature_grids, feature_mappings, geospatial_info_list = geojson_to_numpy_grid_3d_batch(grid_size, geojson_files, features_to_process)\n",
    "\n",
    "# Print results\n",
    "print(\"Shape of the 3D grid array:\", grid_3d.shape)\n",
    "print(\"Feature grids:\", feature_grids.keys())\n",
    "print(\"Feature mappings:\", feature_mappings)\n",
    "print(\"Geospatial information for each file:\", geospatial_info_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='976adc63-d73a-4170-82cf-f577ee085966'>\n",
       "  <div id=\"dda81a2c-4e07-4ba5-a22b-3a0460ad46c6\" data-root-id=\"976adc63-d73a-4170-82cf-f577ee085966\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"eae37642-7f63-49cf-9924-edf13f328f3c\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"976adc63-d73a-4170-82cf-f577ee085966\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"b8b1d553-ec6c-48c6-b62d-331a55dcc50e\",\"attributes\":{\"plot_id\":\"976adc63-d73a-4170-82cf-f577ee085966\",\"comm_id\":\"3436129babe44d819a1e41f348ad1f89\",\"client_comm_id\":\"83ddff070ac2432db5993a578bf1b5ab\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"eae37642-7f63-49cf-9924-edf13f328f3c\",\"roots\":{\"976adc63-d73a-4170-82cf-f577ee085966\":\"dda81a2c-4e07-4ba5-a22b-3a0460ad46c6\"},\"root_ids\":[\"976adc63-d73a-4170-82cf-f577ee085966\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "976adc63-d73a-4170-82cf-f577ee085966"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Layer 1/4: geology_clipped_SUBTIPO_DE\n",
      "Min value in layer: 0.0, Max value in layer: 4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824f4030fa414a7f8c96c04820594894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'46ca4986-3a2c-4676-88ff-137cab8b6ac3': {'version…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Layer 2/4: geology_clipped_GEOCHRON_A\n",
      "Min value in layer: 0.0, Max value in layer: 2.0\n",
      "Plotting Layer 3/4: geology_clipped_UNIDAD_GEN\n",
      "Min value in layer: 0.0, Max value in layer: 26.0\n",
      "Plotting Layer 4/4: possible_anomalies_clipped_fid\n",
      "Min value in layer: nan, Max value in layer: nan\n",
      "Plotting Layer 3/4: geology_clipped_UNIDAD_GEN\n",
      "Min value in layer: 0.0, Max value in layer: 26.0\n",
      "Plotting Layer 1/4: geology_clipped_SUBTIPO_DE\n",
      "Min value in layer: 0.0, Max value in layer: 4.0\n"
     ]
    }
   ],
   "source": [
    "#PLOT\n",
    "\n",
    "# Initialize the Panel extension\n",
    "pn.extension()\n",
    "\n",
    "# Function to plot a specific layer using Matplotlib\n",
    "def plot_layer_bokeh(layer_index):\n",
    "    # Debugging: Print information about the current layer being plotted\n",
    "    print(f\"Plotting Layer {layer_index + 1}/{grid_3d.shape[0]}: {list(feature_grids.keys())[layer_index]}\")\n",
    "    print(f\"Min value in layer: {np.min(grid_3d[layer_index])}, Max value in layer: {np.max(grid_3d[layer_index])}\")\n",
    "\n",
    "    # Create the plot\n",
    "    fig = Figure(figsize=(3, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(grid_3d[layer_index], cmap='tab20', interpolation='nearest', aspect='auto')\n",
    "    ax.set_title(f\"Layer {layer_index + 1}: {list(feature_grids.keys())[layer_index]}\")\n",
    "    fig.colorbar(im, ax=ax, label='Classes')\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "    # Debugging: Display the array data for the current layer\n",
    "    #print(\"Layer data:\\n\", grid_3d[layer_index])\n",
    "\n",
    "    return pn.pane.Matplotlib(fig, tight=True)\n",
    "\n",
    "# Create a Panel widget for selecting the layer\n",
    "layer_slider = pn.widgets.IntSlider(name='Layer Index', start=0, end=grid_3d.shape[0] - 1, step=1, value=0)\n",
    "\n",
    "# Bind the plotting function to the slider value\n",
    "panel = pn.bind(plot_layer_bokeh, layer_index=layer_slider)\n",
    "\n",
    "# Display the Panel with the slider and plot\n",
    "pn.Column(layer_slider, panel).servable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the array to a file\n",
    "np.save(\"C:\\Users\\TyHow\\Documents\\3. Work\\ML_test_area\\exports\\export_3d.npy\", grid_3d)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
