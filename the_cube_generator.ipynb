{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PACKAGE IMPORTS\n",
    "\n",
    "from typing import Tuple, Dict, Any, List\n",
    "import os\n",
    "from tkinter import Tk, filedialog, simpledialog, messagebox, Button, Label\n",
    "import tkinter as tk\n",
    "from tkinter.filedialog import askopenfilenames, askdirectory, asksaveasfilename, askopenfilename\n",
    "from tkinter.simpledialog import askinteger\n",
    "\n",
    "import geopandas as gpd\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.features import rasterize\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import box\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from shapely.geometry import LineString\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 13:39:09.798 python[77702:7816356] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-10-17 13:39:10.382 python[77702:7816356] The class 'NSOpenPanel' overrides the method identifier.  This method is implemented by class 'NSWindow'\n",
      "2024-10-17 13:39:27.538 python[77702:7816356] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Mask file: /Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/pilot_map_testing_mask.geojson\n",
      "Selected GeoJSON files: ['/Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/map1_geology.geojson', '/Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/map3_geology.geojson']\n",
      "Selected Line GeoJSON files: ['/Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/MAP1_faults.geojson', '/Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/MAP3_faults.geojson']\n",
      "Selected Target files: []\n",
      "Selected Raster files: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afac9982b564070ba8615e9d6d1ed62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Select columns for map1_geology.geojson:', options=('lithology', 'age', 'lithology…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092348da2fd045fb9c0ddc3dce95eeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit Selection', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae69764d0f95408d9d4923f04333a606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Select columns for map3_geology.geojson:', options=('lithology', 'age', 'lithology…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98de07ccca67411c80e6bb9aac6c5e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit Selection', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns from /Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/map1_geology.geojson: [('/Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/map1_geology.geojson', 'lithology')]\n",
      "Selected columns from /Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/map3_geology.geojson: [('/Users/thowe/MinersAI Dropbox/Tyler Howe/ML_Pilot_Tyler_Data/geospatial_data/map3_geology.geojson', 'lithology')]\n"
     ]
    }
   ],
   "source": [
    "#UNIFIED FILE IMPORTS\n",
    "\n",
    "def select_files(title, filetypes, multiple=True):\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "    if multiple:\n",
    "        file_paths = filedialog.askopenfilenames(title=title, filetypes=filetypes)\n",
    "    else:\n",
    "        file_paths = filedialog.askopenfilename(title=title, filetypes=filetypes)\n",
    "    root.destroy()\n",
    "    return list(file_paths) if multiple else file_paths\n",
    "\n",
    "# Select all required files in one popup\n",
    "def select_all_files():\n",
    "    mask_file = select_files(\"Select a Mask GeoJSON File\", [(\"GeoJSON files\", \"*.geojson\"), (\"All files\", \"*.*\")], multiple=False)\n",
    "    geojson_files = select_files(\"Select GeoJSON Files\", [(\"GeoJSON files\", \"*.geojson\"), (\"All files\", \"*.*\")])\n",
    "    line_geojson_files = select_files(\"Select GeoJSON Line Files\", [(\"GeoJSON files\", \"*.geojson\"), (\"All files\", \"*.*\")])\n",
    "    target_files = select_files(\"Select Target GeoJSON Files\", [(\"GeoJSON files\", \"*.geojson\"), (\"All files\", \"*.*\")])\n",
    "    raster_files = select_files(\"Select Raster Files\", [(\"GeoTIFF files\", \"*.tif\"), (\"All files\", \"*.*\")])\n",
    "    return mask_file, geojson_files, line_geojson_files, target_files, raster_files\n",
    "\n",
    "# Use the function to select all files\n",
    "mask_file, geojson_files, line_geojson_files, target_files, raster_files = select_all_files()\n",
    "\n",
    "print(f\"Selected Mask file: {mask_file}\")\n",
    "print(f\"Selected GeoJSON files: {geojson_files}\")\n",
    "print(f\"Selected Line GeoJSON files: {line_geojson_files}\")\n",
    "print(f\"Selected Target files: {target_files}\")\n",
    "print(f\"Selected Raster files: {raster_files}\")\n",
    "\n",
    "# Select columns from vector files\n",
    "vector_features_to_process = []\n",
    "\n",
    "def select_columns(geojson_file):\n",
    "    \"\"\"Display column selection widgets for a given GeoJSON file.\"\"\"\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    columns = gdf.columns.tolist()\n",
    "    selection = widgets.SelectMultiple(\n",
    "        options=columns,\n",
    "        description=f'Select columns for {os.path.basename(geojson_file)}:',\n",
    "        rows=10\n",
    "    )\n",
    "    display(selection)\n",
    "\n",
    "    def on_button_click(b):\n",
    "        selected_columns = [(geojson_file, col) for col in selection.value]\n",
    "        vector_features_to_process.extend(selected_columns)\n",
    "        print(f'Selected columns from {geojson_file}: {selected_columns}')\n",
    "\n",
    "    button = widgets.Button(description=\"Submit Selection\")\n",
    "    button.on_click(on_button_click)\n",
    "    display(button)\n",
    "\n",
    "for file in geojson_files:\n",
    "    select_columns(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated grid size: (100, 189)\n"
     ]
    }
   ],
   "source": [
    "#CALCULATE GRID SIZE (from mask short edge)\n",
    "\n",
    "# Function to compute grid size based on the mask file\n",
    "def compute_grid_size(geojson_file: str, short_edge_cells: int = 20) -> Tuple[int, int]:\n",
    "    # Read the GeoJSON file using GeoPandas\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    \n",
    "    # Get the bounding box of the masking region\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    \n",
    "    # Calculate width and height of the bounding box\n",
    "    width = maxx - minx\n",
    "    height = maxy - miny\n",
    "\n",
    "    # Determine which is the short and long edge\n",
    "    if width < height:\n",
    "        short_edge = width\n",
    "        long_edge = height\n",
    "        orientation = 'portrait'\n",
    "    else:\n",
    "        short_edge = height\n",
    "        long_edge = width\n",
    "        orientation = 'landscape'\n",
    "\n",
    "    # Compute the aspect ratio\n",
    "    aspect_ratio = long_edge / short_edge\n",
    "\n",
    "    # Compute the number of cells for the long edge\n",
    "    long_edge_cells = int(short_edge_cells * aspect_ratio)\n",
    "\n",
    "    # Determine the grid size based on the orientation\n",
    "    if orientation == 'portrait':\n",
    "        grid_size = (short_edge_cells, long_edge_cells)\n",
    "    else:\n",
    "        grid_size = (long_edge_cells, short_edge_cells)\n",
    "\n",
    "    return grid_size\n",
    "\n",
    "# Prompt the user for the short_edge_cells value using tkinter\n",
    "root = Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "root.attributes(\"-topmost\", True)  # Ensure it is on top\n",
    "\n",
    "\n",
    "# Ask the user for the short edge size\n",
    "short_edge_cells = simpledialog.askinteger(\"Input\", \"Enter the number of cells for the short edge:\", minvalue=1)\n",
    "\n",
    "root.destroy()  # Close the tkinter root window\n",
    "\n",
    "if short_edge_cells is None:\n",
    "    raise ValueError(\"You must enter a valid number for the short edge size.\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute grid size using the mask file\n",
    "grid_size = compute_grid_size(mask_file, short_edge_cells=short_edge_cells)[::-1]\n",
    "print(f\"Calculated grid size: {grid_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No target files detected.\n",
      "No raster files detected.\n",
      "Reprojecting to a UTM CRS for accurate distance buffering...\n",
      "Buffer distance in pixels: 7\n",
      "Reprojecting to a UTM CRS for accurate distance buffering...\n",
      "Line vector data shape: (2, 100, 189)\n",
      "Vector data shape: (2, 100, 189)\n",
      "vector_data shape: (2, 100, 189)\n",
      "line_vector_data shape: (2, 100, 189)\n",
      "raster_data shape: (0,)\n",
      "target_data shape: (0,)\n",
      "Raster data is empty.\n",
      "Target data is empty.\n",
      "Combined array shape (with targets): (4, 100, 189)\n",
      "Layer Name Mapping List: ['map1_geology_lithology', 'map3_geology_lithology', 'LINE_MAP1_faults', 'LINE_MAP3_faults']\n",
      "Layer name mapping successful. Total layers: 4\n"
     ]
    }
   ],
   "source": [
    "# UNIFIED PROCESSING FUNCTION WITH COMMON GRID ALIGNMENT\n",
    "def process_file(file, file_type, grid_size, mask_file=None, vector_features_to_process=None, buffer_lines=None, buffer_distance_meters=None):\n",
    "    # Define grid bounds based on the mask file\n",
    "    mask_gdf = gpd.read_file(mask_file)\n",
    "    mask_gdf = mask_gdf.to_crs(\"EPSG:3857\")  # Ensure a projected CRS for mask\n",
    "    minx, miny, maxx, maxy = mask_gdf.total_bounds\n",
    "\n",
    "    # Debug step to check the bounds\n",
    "    #print(f\"Grid bounds - minx: {minx}, miny: {miny}, maxx: {maxx}, maxy: {maxy}\")\n",
    "\n",
    "    # Define the transform for the entire grid area based on the mask bounds\n",
    "    common_transform = from_bounds(minx, miny, maxx, maxy, grid_size[1], grid_size[0])\n",
    "\n",
    "    # Debug step to verify function inputs\n",
    "    #print(f\"Buffer lines: {buffer_lines}, Buffer distance meters: {buffer_distance_meters}\")\n",
    "\n",
    "    if file_type == 'target':\n",
    "        # Buffer and rasterize targets\n",
    "        target_buffer_size = get_buffer_size()\n",
    "        target_df = gpd.read_file(file)\n",
    "        target_df_projected = target_df.to_crs(\"EPSG:3857\")\n",
    "        target_df_projected['geometry'] = target_df_projected.geometry.buffer(target_buffer_size)\n",
    "        target_geometry_generator = ((geom, 1) for geom in target_df_projected.geometry)\n",
    "        target_data = rasterize(shapes=target_geometry_generator, out_shape=grid_size, fill=0, transform=common_transform).astype('float32')\n",
    "        target_data_3D = np.expand_dims(target_data, axis=0)\n",
    "        target_layer_name = f\"TARGET_{os.path.basename(file).replace('.geojson', '')}\"\n",
    "        return target_data_3D, target_layer_name\n",
    "\n",
    "    elif file_type == 'vector':\n",
    "        # Process vector data to create numpy grid\n",
    "        gdf = gpd.read_file(file)\n",
    "        gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "        feature_columns = [col for _, col in vector_features_to_process if _ == file]\n",
    "        vector_layers = []\n",
    "        vector_layer_names = []\n",
    "        for feature_column in feature_columns:\n",
    "            unique_categories = gdf[feature_column].unique()\n",
    "            category_to_int = {cat: i for i, cat in enumerate(unique_categories)}\n",
    "            sindex = gdf.sindex\n",
    "            x = np.linspace(minx, maxx, grid_size[1] + 1)\n",
    "            y = np.linspace(miny, maxy, grid_size[0] + 1)\n",
    "            cells = [box(x[j], y[i], x[j + 1], y[i + 1]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "            results = Parallel(n_jobs=-1)(delayed(process_cell)(idx, cell, gdf, sindex, feature_column, category_to_int, grid_size) for idx, cell in enumerate(cells))\n",
    "            grid = np.full(grid_size, np.nan)\n",
    "            for i, j, value in results:\n",
    "                grid[i, j] = value\n",
    "            grid_flipped = np.flipud(grid)\n",
    "            vector_layers.append(np.expand_dims(grid_flipped, axis=0))\n",
    "            vector_layer_names.append(f\"{os.path.basename(file).replace('.geojson', '')}_{feature_column}\")\n",
    "        return np.concatenate(vector_layers, axis=0), vector_layer_names\n",
    "\n",
    "    elif file_type == 'line':\n",
    "        # Process line vector files with buffering and distance calculation\n",
    "        buffer_lines = user_buffer_choice(file)\n",
    "        buffer_distance_meters = get_buffer_distance_meters(file) if buffer_lines else 0  # Pass 'file' to get_buffer_distance_meters\n",
    "        gdf = gpd.read_file(file)\n",
    "\n",
    "        # Reproject to a projected CRS if necessary (e.g., UTM)\n",
    "        if not gdf.crs or not gdf.crs.is_projected:\n",
    "            print(\"Reprojecting to a UTM CRS for accurate distance buffering...\")\n",
    "            gdf = gdf.to_crs(gdf.estimate_utm_crs())\n",
    "\n",
    "        # Calculate pixel distance if buffering is enabled\n",
    "        pixel_distance = 0\n",
    "        if buffer_lines:\n",
    "            pixel_distance = calculate_pixel_distance(gdf, grid_size, buffer_distance_meters)\n",
    "            print(f\"Buffer distance in pixels: {pixel_distance}\")\n",
    "\n",
    "        shapes = []\n",
    "\n",
    "        # Iterate through each geometry, ensuring it is not None\n",
    "        for geom in gdf.geometry:\n",
    "            if geom is None:\n",
    "                continue  # Skip None geometries\n",
    "            if geom.geom_type == 'LineString':\n",
    "                shapes.append((geom, 1))\n",
    "            elif geom.geom_type == 'MultiLineString':\n",
    "                for line in geom.geoms:\n",
    "                    if isinstance(line, LineString):\n",
    "                        shapes.append((line, 1))\n",
    "\n",
    "        # Rasterize the geometries to create the binary grid\n",
    "        if shapes:\n",
    "            binary_grid = rasterize(shapes, out_shape=grid_size, transform=common_transform, fill=0, dtype='uint8')\n",
    "\n",
    "            if buffer_lines and pixel_distance > 0:\n",
    "                raster_map = calculate_distance(binary_grid, max_distance=pixel_distance)\n",
    "            else:\n",
    "                raster_map = binary_grid.astype(np.float32)\n",
    "        else:\n",
    "            print(f\"No valid line geometries found in {file}. Skipping...\")\n",
    "            raster_map = np.zeros(grid_size, dtype=np.float32)\n",
    "\n",
    "        line_layer_name = f\"LINE_{os.path.basename(file).replace('.geojson', '')}\"\n",
    "        return np.expand_dims(raster_map, axis=0), line_layer_name\n",
    "\n",
    "    \n",
    "    elif file_type == 'raster':\n",
    "        # Process raster files\n",
    "        if not mask_file:\n",
    "            print(\"No mask file provided. Skipping raster processing.\")\n",
    "            return None, None\n",
    "        mask_gdf = gpd.read_file(mask_file)\n",
    "        minx, miny, maxx, maxy = mask_gdf.total_bounds\n",
    "        raster_target_transform = from_bounds(minx, miny, maxx, maxy, grid_size[1], grid_size[0])\n",
    "        raster_target_crs = \"EPSG:4326\"\n",
    "        with rasterio.open(file, 'r') as src:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            src_crs = src.crs if src.crs != raster_target_crs else raster_target_crs\n",
    "            raster_data_array = np.full(grid_size, np.nan, dtype=np.float32)\n",
    "            nodata_value = src.nodata if src.nodata is not None else np.nan\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=raster_data_array,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src_crs,\n",
    "                dst_transform=common_transform,\n",
    "                dst_crs=raster_target_crs,\n",
    "                resampling=Resampling.nearest,\n",
    "                src_nodata=nodata_value,\n",
    "                dst_nodata=np.nan\n",
    "            )\n",
    "        raster_name = os.path.basename(file).replace('.tiff', '').replace('.tif', '')\n",
    "        return np.expand_dims(raster_data_array, axis=0), raster_name\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type provided.\")\n",
    "\n",
    "# Helper functions\n",
    "def get_buffer_size():\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "    buffer_size = simpledialog.askinteger(\"Input\", \"Enter the target buffer size in meters:\", minvalue=1)\n",
    "    root.destroy()\n",
    "    return buffer_size\n",
    "\n",
    "def process_cell(idx, cell, gdf, sindex, feature_column, category_to_int, grid_size):\n",
    "    i, j = divmod(idx, grid_size[1])\n",
    "    possible_matches_index = list(sindex.intersection(cell.bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    intersections = possible_matches.geometry.intersection(cell)\n",
    "    valid_intersections = intersections[intersections.area > 0]\n",
    "    if valid_intersections.empty:\n",
    "        return i, j, np.nan\n",
    "    largest_intersection_idx = valid_intersections.area.idxmax()\n",
    "    category = possible_matches.loc[largest_intersection_idx, feature_column]\n",
    "    return i, j, category_to_int[category]\n",
    "\n",
    "def calculate_distance(arr, max_distance=20, dtype=np.float32):\n",
    "    arr = np.asarray(arr, dtype=bool)\n",
    "    dist = distance_transform_edt(~arr)\n",
    "    normalized_dist = np.clip(1 - dist / max_distance, 0, 1)\n",
    "    return normalized_dist.astype(dtype)\n",
    "\n",
    "# Function to calculate pixel distance from buffer distance in meters\n",
    "def calculate_pixel_distance(gdf, grid_size, buffer_distance_meters):\n",
    "    height, width = grid_size\n",
    "    bounds = gdf.total_bounds\n",
    "    x_range_meters = bounds[2] - bounds[0]\n",
    "    y_range_meters = bounds[3] - bounds[1]\n",
    "    x_res = x_range_meters / width\n",
    "    y_res = y_range_meters / height\n",
    "    pixel_distance = buffer_distance_meters / ((x_res + y_res) / 2)\n",
    "    return int(pixel_distance)\n",
    "\n",
    "# Function for user to choose between buffering or rasterizing\n",
    "def user_buffer_choice(file):\n",
    "    choice = {\"buffer\": None}\n",
    "\n",
    "    def set_choice_buffer():\n",
    "        choice[\"buffer\"] = True\n",
    "        window.destroy()\n",
    "\n",
    "    def set_choice_rasterize():\n",
    "        choice[\"buffer\"] = False\n",
    "        window.destroy()\n",
    "\n",
    "    window = Tk()\n",
    "    window.title(\"Choose Processing Option\")\n",
    "    label = Label(window, text=f\"Would you like to buffer the lines (calculate distances) or just rasterize them for {os.path.basename(file)}?\")\n",
    "    label.pack(pady=10)\n",
    "    buffer_button = Button(window, text=\"Buffer (Calculate Distance)\", command=set_choice_buffer)\n",
    "    buffer_button.pack(side=\"left\", padx=20, pady=20)\n",
    "    rasterize_button = Button(window, text=\"Rasterize Only\", command=set_choice_rasterize)\n",
    "    rasterize_button.pack(side=\"right\", padx=20, pady=20)\n",
    "    window.mainloop()\n",
    "    return choice[\"buffer\"]\n",
    "\n",
    "def get_buffer_distance_meters(file):\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    buffer_distance = None\n",
    "    try:\n",
    "        buffer_distance = simpledialog.askfloat(\n",
    "            title=f\"Input Buffer Distance for {os.path.basename(file)}\",\n",
    "            prompt=\"Please enter buffer distance in meters:\"\n",
    "        )\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Invalid Input\", \"Please enter a valid number.\")\n",
    "        buffer_distance = None\n",
    "    root.destroy()\n",
    "    return buffer_distance\n",
    "\n",
    "# Process targets\n",
    "target_data = []\n",
    "target_layer_names = []\n",
    "if target_files:\n",
    "    for file in target_files:\n",
    "        target_layer, target_layer_name = process_file(file, 'target', grid_size, mask_file=mask_file)\n",
    "        target_data.append(target_layer)\n",
    "        target_layer_names.append(target_layer_name)\n",
    "    target_data = np.concatenate(target_data, axis=0)\n",
    "    print(f\"Target data shape: {target_data.shape}\")\n",
    "else:\n",
    "    print(\"No target files detected.\")\n",
    "    target_data = np.array([])\n",
    "    target_layer_names = []\n",
    "\n",
    "# Process rasters\n",
    "raster_data = []\n",
    "raster_names = []\n",
    "if raster_files:\n",
    "    if not mask_file:\n",
    "        print(\"No mask file selected. Skipping raster processing.\")\n",
    "    else:\n",
    "        for file in raster_files:\n",
    "            raster_layer, raster_name = process_file(file, 'raster', grid_size, mask_file=mask_file)\n",
    "            if raster_layer is not None:\n",
    "                raster_data.append(raster_layer)\n",
    "                raster_names.append(raster_name)\n",
    "        if raster_data:\n",
    "            raster_data = np.concatenate(raster_data, axis=0)\n",
    "            print(f\"Raster data shape: {raster_data.shape}\")\n",
    "        else:\n",
    "            print(\"No raster data processed.\")\n",
    "else:\n",
    "    print(\"No raster files detected.\")\n",
    "    raster_data = np.array([])\n",
    "    raster_names = []\n",
    "\n",
    "# Process line vectors\n",
    "line_vector_data = []\n",
    "line_layer_names = []\n",
    "if line_geojson_files:\n",
    "    for file in line_geojson_files:\n",
    "        line_layer, line_layer_name = process_file(file, 'line', grid_size, mask_file=mask_file, buffer_lines=buffer_lines, buffer_distance_meters=buffer_distance_meters)\n",
    "        line_vector_data.append(line_layer)\n",
    "        line_layer_names.append(line_layer_name)\n",
    "    line_vector_data = np.concatenate(line_vector_data, axis=0)\n",
    "    print(f\"Line vector data shape: {line_vector_data.shape}\")\n",
    "else:\n",
    "    print(\"No line vector files selected.\")\n",
    "    line_vector_data = np.array([])\n",
    "    line_layer_names = []\n",
    "\n",
    "# Process polygon vectors\n",
    "vector_data = []\n",
    "vector_feature_grids = {}\n",
    "vector_layer_names = []\n",
    "if geojson_files and vector_features_to_process:\n",
    "    for file in geojson_files:\n",
    "        vector_layer, feature_names = process_file(file, 'vector', grid_size, mask_file=mask_file, vector_features_to_process=vector_features_to_process)\n",
    "        vector_data.append(vector_layer)\n",
    "        vector_layer_names.extend(feature_names)\n",
    "        for feature_name, layer in zip(feature_names, vector_layer):\n",
    "            vector_feature_grids[feature_name] = layer\n",
    "    vector_data = np.concatenate(vector_data, axis=0)\n",
    "    print(f\"Vector data shape: {vector_data.shape}\")\n",
    "else:\n",
    "    print(\"No polygon vector files or features selected.\")\n",
    "    vector_data = np.array([])\n",
    "    vector_feature_grids = {}\n",
    "    vector_layer_names = []\n",
    "\n",
    "print(f\"vector_data shape: {vector_data.shape}\")\n",
    "print(f\"line_vector_data shape: {line_vector_data.shape}\")\n",
    "print(f\"raster_data shape: {raster_data.shape}\")\n",
    "print(f\"target_data shape: {target_data.shape}\")\n",
    "\n",
    "combined_data = []\n",
    "combined_layer_names = []\n",
    "\n",
    "# Create a list to hold non-empty data arrays and their corresponding layer names\n",
    "data_arrays = []\n",
    "layer_names = []\n",
    "\n",
    "# Function to add non-empty data arrays and their names\n",
    "def add_data_and_names(data_array, names):\n",
    "    if data_array.size != 0:\n",
    "        data_arrays.append(data_array)\n",
    "        layer_names.extend(names)\n",
    "    else:\n",
    "        print(f\"Skipping empty data array with names: {names}\")\n",
    "\n",
    "# Add vector data\n",
    "if vector_data.size != 0:\n",
    "    add_data_and_names(vector_data, vector_layer_names)\n",
    "else:\n",
    "    print(\"Vector data is empty.\")\n",
    "\n",
    "# Add raster data\n",
    "if raster_data.size != 0:\n",
    "    add_data_and_names(raster_data, raster_names)\n",
    "else:\n",
    "    print(\"Raster data is empty.\")\n",
    "\n",
    "# Add line vector data\n",
    "if line_vector_data.size != 0:\n",
    "    add_data_and_names(line_vector_data, line_layer_names)\n",
    "else:\n",
    "    print(\"Line vector data is empty.\")\n",
    "\n",
    "# Add target data\n",
    "if target_data.size != 0:\n",
    "    add_data_and_names(target_data, target_layer_names)\n",
    "else:\n",
    "    print(\"Target data is empty.\")\n",
    "\n",
    "# Check if we have at least one non-empty data array\n",
    "if data_arrays:\n",
    "    # Check that all arrays have the same x and y dimensions\n",
    "    first_shape = data_arrays[0].shape[1:]  # Skip the first dimension (number of layers)\n",
    "    shapes_match = all(data_array.shape[1:] == first_shape for data_array in data_arrays)\n",
    "\n",
    "    if shapes_match:\n",
    "        # Concatenate all data arrays along axis=0\n",
    "        combined_data = np.concatenate(data_arrays, axis=0)\n",
    "        combined_layer_names = layer_names\n",
    "\n",
    "        print(f\"Combined array shape (with targets): {combined_data.shape}\")\n",
    "\n",
    "        # Check the mapping to ensure it is correct\n",
    "        print(\"Layer Name Mapping List:\", combined_layer_names)\n",
    "\n",
    "        # Ensure the combined_data layers match the number of names\n",
    "        if len(combined_layer_names) == combined_data.shape[0]:\n",
    "            print(f\"Layer name mapping successful. Total layers: {len(combined_layer_names)}\")\n",
    "        else:\n",
    "            print(f\"Warning: Mismatch in layers. {len(combined_layer_names)} names for {combined_data.shape[0]} layers.\")\n",
    "    else:\n",
    "        print(\"Error: The x/y dimensions of the arrays do not match.\")\n",
    "        print(\"Array shapes:\")\n",
    "        for idx, data_array in enumerate(data_arrays):\n",
    "            print(f\"Array {idx} shape: {data_array.shape}\")\n",
    "else:\n",
    "    print(\"No data arrays to combine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No target files detected.\n",
      "No raster files detected.\n",
      "COMMON TRANSFORM........... | 296.85, 0.00,-7876319.94|\n",
      "| 0.00,-345.93,-3632752.92|\n",
      "| 0.00, 0.00, 1.00|\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'minx_common' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 257\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line_geojson_files:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m line_geojson_files:\n\u001b[0;32m--> 257\u001b[0m         line_layer, line_layer_name \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_distance_meters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_distance_meters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m         line_vector_data\u001b[38;5;241m.\u001b[39mappend(line_layer)\n\u001b[1;32m    259\u001b[0m         line_layer_names\u001b[38;5;241m.\u001b[39mappend(line_layer_name)\n",
      "Cell \u001b[0;32mIn[26], line 63\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file, file_type, grid_size, mask_file, vector_features_to_process, buffer_lines, buffer_distance_meters)\u001b[0m\n\u001b[1;32m     60\u001b[0m gdf \u001b[38;5;241m=\u001b[39m gdf\u001b[38;5;241m.\u001b[39mto_crs(mask_gdf\u001b[38;5;241m.\u001b[39mcrs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Clip the line data to the common bounding box\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m common_bounds \u001b[38;5;241m=\u001b[39m box(\u001b[43mminx_common\u001b[49m, miny_common, maxx_common, maxy_common)\n\u001b[1;32m     64\u001b[0m gdf_clipped \u001b[38;5;241m=\u001b[39m gdf[gdf\u001b[38;5;241m.\u001b[39mintersects(common_bounds)]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# If there's no intersection, skip this file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'minx_common' is not defined"
     ]
    }
   ],
   "source": [
    "# UNIFIED PROCESSING FUNCTION WITH COMMON GRID ALIGNMENT ______2\n",
    "def process_file(file, file_type, grid_size, mask_file=None, vector_features_to_process=None, buffer_lines=None, buffer_distance_meters=None):\n",
    "    # Define grid bounds based on the mask file\n",
    "    mask_gdf = gpd.read_file(mask_file)\n",
    "    mask_gdf = mask_gdf.to_crs(\"EPSG:3857\")  # Ensure a projected CRS for mask\n",
    "    minx, miny, maxx, maxy = mask_gdf.total_bounds\n",
    "\n",
    "    # Debug step to check the bounds\n",
    "    #print(f\"Grid bounds - minx: {minx}, miny: {miny}, maxx: {maxx}, maxy: {maxy}\")\n",
    "\n",
    "    # Define the transform for the entire grid area based on the mask bounds\n",
    "    common_transform = from_bounds(minx, miny, maxx, maxy, grid_size[1], grid_size[0])\n",
    "    print('COMMON TRANSFORM...........', common_transform)\n",
    "\n",
    "    # Debug step to verify function inputs\n",
    "    #print(f\"Buffer lines: {buffer_lines}, Buffer distance meters: {buffer_distance_meters}\")\n",
    "\n",
    "    if file_type == 'target':\n",
    "        # Buffer and rasterize targets\n",
    "        target_buffer_size = get_buffer_size()\n",
    "        target_df = gpd.read_file(file)\n",
    "        target_df_projected = target_df.to_crs(\"EPSG:3857\")\n",
    "        target_df_projected['geometry'] = target_df_projected.geometry.buffer(target_buffer_size)\n",
    "        target_geometry_generator = ((geom, 1) for geom in target_df_projected.geometry)\n",
    "        target_data = rasterize(shapes=target_geometry_generator, out_shape=grid_size, fill=0, transform=common_transform).astype('float32')\n",
    "        target_data_3D = np.expand_dims(target_data, axis=0)\n",
    "        target_layer_name = f\"TARGET_{os.path.basename(file).replace('.geojson', '')}\"\n",
    "        return target_data_3D, target_layer_name\n",
    "\n",
    "    elif file_type == 'vector':\n",
    "        # Process vector data to create numpy grid\n",
    "        gdf = gpd.read_file(file)\n",
    "        gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "        feature_columns = [col for _, col in vector_features_to_process if _ == file]\n",
    "        vector_layers = []\n",
    "        vector_layer_names = []\n",
    "        for feature_column in feature_columns:\n",
    "            unique_categories = gdf[feature_column].unique()\n",
    "            category_to_int = {cat: i for i, cat in enumerate(unique_categories)}\n",
    "            sindex = gdf.sindex\n",
    "            x = np.linspace(minx, maxx, grid_size[1] + 1)\n",
    "            y = np.linspace(miny, maxy, grid_size[0] + 1)\n",
    "            cells = [box(x[j], y[i], x[j + 1], y[i + 1]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "            results = Parallel(n_jobs=-1)(delayed(process_cell)(idx, cell, gdf, sindex, feature_column, category_to_int, grid_size) for idx, cell in enumerate(cells))\n",
    "            grid = np.full(grid_size, np.nan)\n",
    "            for i, j, value in results:\n",
    "                grid[i, j] = value\n",
    "            grid_flipped = np.flipud(grid)\n",
    "            vector_layers.append(np.expand_dims(grid_flipped, axis=0))\n",
    "            vector_layer_names.append(f\"{os.path.basename(file).replace('.geojson', '')}_{feature_column}\")\n",
    "        return np.concatenate(vector_layers, axis=0), vector_layer_names\n",
    "\n",
    "    elif file_type == 'line':\n",
    "        # Process line vector files with buffering and distance calculation\n",
    "        buffer_lines = user_buffer_choice(file)\n",
    "        buffer_distance_meters = get_buffer_distance_meters(file) if buffer_lines else 0\n",
    "        gdf = gpd.read_file(file)\n",
    "    \n",
    "        # Reproject to match the CRS of the mask (common grid)\n",
    "        gdf = gdf.to_crs(mask_gdf.crs)\n",
    "    \n",
    "        # Clip the line data to the common bounding box\n",
    "        common_bounds = box(minx, miny, maxx, maxy)\n",
    "        gdf_clipped = gdf[gdf.intersects(common_bounds)]\n",
    "    \n",
    "        # If there's no intersection, skip this file\n",
    "        if gdf_clipped.empty:\n",
    "            print(f\"Warning: No valid area for line data within the common grid for file {file}. Skipping...\")\n",
    "            raster_map = np.zeros(grid_size, dtype=np.float32)\n",
    "            line_layer_name = f\"LINE_{os.path.basename(file).replace('.geojson', '')}\"\n",
    "            return np.expand_dims(raster_map, axis=0), line_layer_name\n",
    "    \n",
    "        # Calculate the correct transform for the common grid placement\n",
    "        y_res = abs((maxy - miny) / grid_size[0])\n",
    "        x_res = abs((maxx - minx) / grid_size[1])\n",
    "    \n",
    "        row_start = max(0, int((maxy - gdf_clipped.total_bounds[3]) / y_res))\n",
    "        row_end = min(grid_size[0], int((maxy - gdf_clipped.total_bounds[1]) / y_res))\n",
    "        col_start = max(0, int((gdf_clipped.total_bounds[0] - minx) / x_res))\n",
    "        col_end = min(grid_size[1], int((gdf_clipped.total_bounds[2] - minx) / x_res))\n",
    "    \n",
    "        # Define the transform for rasterizing within the clipped bounds\n",
    "        line_transform = rasterio.transform.from_bounds(\n",
    "            *gdf_clipped.total_bounds,\n",
    "            col_end - col_start,\n",
    "            row_end - row_start\n",
    "        )\n",
    "    \n",
    "        # Rasterize the clipped geometries\n",
    "        shapes = [(geom, 1) for geom in gdf_clipped.geometry if geom is not None]\n",
    "        if shapes:\n",
    "            line_raster = rasterize(\n",
    "                shapes=shapes,\n",
    "                out_shape=(row_end - row_start, col_end - col_start),\n",
    "                transform=line_transform,\n",
    "                fill=0,\n",
    "                dtype='uint8'\n",
    "            )\n",
    "            \n",
    "            # Create a full grid for the common grid and place the line raster in the correct position\n",
    "            binary_grid = np.zeros(grid_size, dtype=np.uint8)\n",
    "            binary_grid[row_start:row_end, col_start:col_end] = line_raster\n",
    "        else:\n",
    "            print(f\"No valid line geometries found in {file}. Skipping...\")\n",
    "            binary_grid = np.zeros(grid_size, dtype=np.float32)\n",
    "    \n",
    "        line_layer_name = f\"LINE_{os.path.basename(file).replace('.geojson', '')}\"\n",
    "        return np.expand_dims(binary_grid.astype(np.float32), axis=0), line_layer_name\n",
    "\n",
    "\n",
    "    \n",
    "    elif file_type == 'raster':\n",
    "        # Process raster files\n",
    "        if not mask_file:\n",
    "            print(\"No mask file provided. Skipping raster processing.\")\n",
    "            return None, None\n",
    "        mask_gdf = gpd.read_file(mask_file)\n",
    "        minx, miny, maxx, maxy = mask_gdf.total_bounds\n",
    "        raster_target_transform = from_bounds(minx, miny, maxx, maxy, grid_size[1], grid_size[0])\n",
    "        raster_target_crs = \"EPSG:4326\"\n",
    "        with rasterio.open(file, 'r') as src:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            src_crs = src.crs if src.crs != raster_target_crs else raster_target_crs\n",
    "            raster_data_array = np.full(grid_size, np.nan, dtype=np.float32)\n",
    "            nodata_value = src.nodata if src.nodata is not None else np.nan\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=raster_data_array,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src_crs,\n",
    "                dst_transform=common_transform,\n",
    "                dst_crs=raster_target_crs,\n",
    "                resampling=Resampling.nearest,\n",
    "                src_nodata=nodata_value,\n",
    "                dst_nodata=np.nan\n",
    "            )\n",
    "        raster_name = os.path.basename(file).replace('.tiff', '').replace('.tif', '')\n",
    "        return np.expand_dims(raster_data_array, axis=0), raster_name\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type provided.\")\n",
    "\n",
    "# Helper functions\n",
    "def get_buffer_size():\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "    buffer_size = simpledialog.askinteger(\"Input\", \"Enter the target buffer size in meters:\", minvalue=1)\n",
    "    root.destroy()\n",
    "    return buffer_size\n",
    "\n",
    "def process_cell(idx, cell, gdf, sindex, feature_column, category_to_int, grid_size):\n",
    "    i, j = divmod(idx, grid_size[1])\n",
    "    possible_matches_index = list(sindex.intersection(cell.bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    intersections = possible_matches.geometry.intersection(cell)\n",
    "    valid_intersections = intersections[intersections.area > 0]\n",
    "    if valid_intersections.empty:\n",
    "        return i, j, np.nan\n",
    "    largest_intersection_idx = valid_intersections.area.idxmax()\n",
    "    category = possible_matches.loc[largest_intersection_idx, feature_column]\n",
    "    return i, j, category_to_int[category]\n",
    "\n",
    "def calculate_distance(arr, max_distance=20, dtype=np.float32):\n",
    "    arr = np.asarray(arr, dtype=bool)\n",
    "    dist = distance_transform_edt(~arr)\n",
    "    normalized_dist = np.clip(1 - dist / max_distance, 0, 1)\n",
    "    return normalized_dist.astype(dtype)\n",
    "\n",
    "# Function to calculate pixel distance from buffer distance in meters\n",
    "def calculate_pixel_distance(gdf, grid_size, buffer_distance_meters):\n",
    "    height, width = grid_size\n",
    "    bounds = gdf.total_bounds\n",
    "    x_range_meters = bounds[2] - bounds[0]\n",
    "    y_range_meters = bounds[3] - bounds[1]\n",
    "    x_res = x_range_meters / width\n",
    "    y_res = y_range_meters / height\n",
    "    pixel_distance = buffer_distance_meters / ((x_res + y_res) / 2)\n",
    "    return int(pixel_distance)\n",
    "\n",
    "# Function for user to choose between buffering or rasterizing\n",
    "def user_buffer_choice(file):\n",
    "    choice = {\"buffer\": None}\n",
    "\n",
    "    def set_choice_buffer():\n",
    "        choice[\"buffer\"] = True\n",
    "        window.destroy()\n",
    "\n",
    "    def set_choice_rasterize():\n",
    "        choice[\"buffer\"] = False\n",
    "        window.destroy()\n",
    "\n",
    "    window = Tk()\n",
    "    window.title(\"Choose Processing Option\")\n",
    "    label = Label(window, text=f\"Would you like to buffer the lines (calculate distances) or just rasterize them for {os.path.basename(file)}?\")\n",
    "    label.pack(pady=10)\n",
    "    buffer_button = Button(window, text=\"Buffer (Calculate Distance)\", command=set_choice_buffer)\n",
    "    buffer_button.pack(side=\"left\", padx=20, pady=20)\n",
    "    rasterize_button = Button(window, text=\"Rasterize Only\", command=set_choice_rasterize)\n",
    "    rasterize_button.pack(side=\"right\", padx=20, pady=20)\n",
    "    window.mainloop()\n",
    "    return choice[\"buffer\"]\n",
    "\n",
    "def get_buffer_distance_meters(file):\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    buffer_distance = None\n",
    "    try:\n",
    "        buffer_distance = simpledialog.askfloat(\n",
    "            title=f\"Input Buffer Distance for {os.path.basename(file)}\",\n",
    "            prompt=\"Please enter buffer distance in meters:\"\n",
    "        )\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Invalid Input\", \"Please enter a valid number.\")\n",
    "        buffer_distance = None\n",
    "    root.destroy()\n",
    "    return buffer_distance\n",
    "\n",
    "# Process targets\n",
    "target_data = []\n",
    "target_layer_names = []\n",
    "if target_files:\n",
    "    for file in target_files:\n",
    "        target_layer, target_layer_name = process_file(file, 'target', grid_size, mask_file=mask_file)\n",
    "        target_data.append(target_layer)\n",
    "        target_layer_names.append(target_layer_name)\n",
    "    target_data = np.concatenate(target_data, axis=0)\n",
    "    print(f\"Target data shape: {target_data.shape}\")\n",
    "else:\n",
    "    print(\"No target files detected.\")\n",
    "    target_data = np.array([])\n",
    "    target_layer_names = []\n",
    "\n",
    "# Process rasters\n",
    "raster_data = []\n",
    "raster_names = []\n",
    "if raster_files:\n",
    "    if not mask_file:\n",
    "        print(\"No mask file selected. Skipping raster processing.\")\n",
    "    else:\n",
    "        for file in raster_files:\n",
    "            raster_layer, raster_name = process_file(file, 'raster', grid_size, mask_file=mask_file)\n",
    "            if raster_layer is not None:\n",
    "                raster_data.append(raster_layer)\n",
    "                raster_names.append(raster_name)\n",
    "        if raster_data:\n",
    "            raster_data = np.concatenate(raster_data, axis=0)\n",
    "            print(f\"Raster data shape: {raster_data.shape}\")\n",
    "        else:\n",
    "            print(\"No raster data processed.\")\n",
    "else:\n",
    "    print(\"No raster files detected.\")\n",
    "    raster_data = np.array([])\n",
    "    raster_names = []\n",
    "\n",
    "# Process line vectors\n",
    "line_vector_data = []\n",
    "line_layer_names = []\n",
    "if line_geojson_files:\n",
    "    for file in line_geojson_files:\n",
    "        line_layer, line_layer_name = process_file(file, 'line', grid_size, mask_file=mask_file, buffer_lines=buffer_lines, buffer_distance_meters=buffer_distance_meters)\n",
    "        line_vector_data.append(line_layer)\n",
    "        line_layer_names.append(line_layer_name)\n",
    "    line_vector_data = np.concatenate(line_vector_data, axis=0)\n",
    "    print(f\"Line vector data shape: {line_vector_data.shape}\")\n",
    "else:\n",
    "    print(\"No line vector files selected.\")\n",
    "    line_vector_data = np.array([])\n",
    "    line_layer_names = []\n",
    "\n",
    "# Process polygon vectors\n",
    "vector_data = []\n",
    "vector_feature_grids = {}\n",
    "vector_layer_names = []\n",
    "if geojson_files and vector_features_to_process:\n",
    "    for file in geojson_files:\n",
    "        vector_layer, feature_names = process_file(file, 'vector', grid_size, mask_file=mask_file, vector_features_to_process=vector_features_to_process)\n",
    "        vector_data.append(vector_layer)\n",
    "        vector_layer_names.extend(feature_names)\n",
    "        for feature_name, layer in zip(feature_names, vector_layer):\n",
    "            vector_feature_grids[feature_name] = layer\n",
    "    vector_data = np.concatenate(vector_data, axis=0)\n",
    "    print(f\"Vector data shape: {vector_data.shape}\")\n",
    "else:\n",
    "    print(\"No polygon vector files or features selected.\")\n",
    "    vector_data = np.array([])\n",
    "    vector_feature_grids = {}\n",
    "    vector_layer_names = []\n",
    "\n",
    "print(f\"vector_data shape: {vector_data.shape}\")\n",
    "print(f\"line_vector_data shape: {line_vector_data.shape}\")\n",
    "print(f\"raster_data shape: {raster_data.shape}\")\n",
    "print(f\"target_data shape: {target_data.shape}\")\n",
    "\n",
    "combined_data = []\n",
    "combined_layer_names = []\n",
    "\n",
    "# Create a list to hold non-empty data arrays and their corresponding layer names\n",
    "data_arrays = []\n",
    "layer_names = []\n",
    "\n",
    "# Function to add non-empty data arrays and their names\n",
    "def add_data_and_names(data_array, names):\n",
    "    if data_array.size != 0:\n",
    "        data_arrays.append(data_array)\n",
    "        layer_names.extend(names)\n",
    "    else:\n",
    "        print(f\"Skipping empty data array with names: {names}\")\n",
    "\n",
    "# Add vector data\n",
    "if vector_data.size != 0:\n",
    "    add_data_and_names(vector_data, vector_layer_names)\n",
    "else:\n",
    "    print(\"Vector data is empty.\")\n",
    "\n",
    "# Add raster data\n",
    "if raster_data.size != 0:\n",
    "    add_data_and_names(raster_data, raster_names)\n",
    "else:\n",
    "    print(\"Raster data is empty.\")\n",
    "\n",
    "# Add line vector data\n",
    "if line_vector_data.size != 0:\n",
    "    add_data_and_names(line_vector_data, line_layer_names)\n",
    "else:\n",
    "    print(\"Line vector data is empty.\")\n",
    "\n",
    "# Add target data\n",
    "if target_data.size != 0:\n",
    "    add_data_and_names(target_data, target_layer_names)\n",
    "else:\n",
    "    print(\"Target data is empty.\")\n",
    "\n",
    "# Check if we have at least one non-empty data array\n",
    "if data_arrays:\n",
    "    # Check that all arrays have the same x and y dimensions\n",
    "    first_shape = data_arrays[0].shape[1:]  # Skip the first dimension (number of layers)\n",
    "    shapes_match = all(data_array.shape[1:] == first_shape for data_array in data_arrays)\n",
    "\n",
    "    if shapes_match:\n",
    "        # Concatenate all data arrays along axis=0\n",
    "        combined_data = np.concatenate(data_arrays, axis=0)\n",
    "        combined_layer_names = layer_names\n",
    "\n",
    "        print(f\"Combined array shape (with targets): {combined_data.shape}\")\n",
    "\n",
    "        # Check the mapping to ensure it is correct\n",
    "        print(\"Layer Name Mapping List:\", combined_layer_names)\n",
    "\n",
    "        # Ensure the combined_data layers match the number of names\n",
    "        if len(combined_layer_names) == combined_data.shape[0]:\n",
    "            print(f\"Layer name mapping successful. Total layers: {len(combined_layer_names)}\")\n",
    "        else:\n",
    "            print(f\"Warning: Mismatch in layers. {len(combined_layer_names)} names for {combined_data.shape[0]} layers.\")\n",
    "    else:\n",
    "        print(\"Error: The x/y dimensions of the arrays do not match.\")\n",
    "        print(\"Array shapes:\")\n",
    "        for idx, data_array in enumerate(data_arrays):\n",
    "            print(f\"Array {idx} shape: {data_array.shape}\")\n",
    "else:\n",
    "    print(\"No data arrays to combine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT DATA ////////// SAVE RASTER & VECTOR DATA\n",
    "\n",
    "# Hide the root window for file dialog\n",
    "root = Tk()\n",
    "root.withdraw()  # Hide the main window\n",
    "root.attributes(\"-topmost\", True)  # Bring the file dialog to the front\n",
    "\n",
    "# Prompt the user to select a folder to save the files\n",
    "output_directory = askdirectory(\n",
    "    initialdir=r\"C:\\Users\\TyHow\\Documents\\3. Work\\ML_test_area\\exports\",\n",
    "    title=\"Select a Folder to Save Output Files (BEWARE OVERWRITE!)\"\n",
    ")\n",
    "\n",
    "if output_directory:\n",
    "    # Construct file paths using the selected folder and default file names\n",
    "    output_6_rasters_file = os.path.join(output_directory, \"output_rasters.npy\")\n",
    "    output_6_rasters_layer_mappings_file = os.path.join(output_directory, \"output_rasters_layer_mappings.npy\")\n",
    "\n",
    "    # Save the files\n",
    "    np.save(output_6_rasters_file, raster_data)\n",
    "    np.save(output_6_rasters_layer_mappings_file, raster_feature_mappings)\n",
    "\n",
    "    print(f\"Files saved in: {output_directory}\")\n",
    "\n",
    "# Destroy the root window after file dialogs are closed\n",
    "root.destroy()\n",
    "\n",
    "# Hide the root window for file dialog\n",
    "root = Tk()\n",
    "root.withdraw()  # Hide the main window\n",
    "root.attributes(\"-topmost\", True)  # Bring the file dialog to the front\n",
    "\n",
    "# Prompt the user to select a folder to save the files\n",
    "output_directory = askdirectory(\n",
    "    initialdir=r\"C:\\Users\\TyHow\\Documents\\3. Work\\ML_test_area\\exports\",\n",
    "    title=\"Select a Folder to Save Output Files (BEWARE OVERWRITE!)\"\n",
    ")\n",
    "\n",
    "if output_directory:\n",
    "    # Construct file paths using the selected folder and default file names\n",
    "    output_array_file = os.path.join(output_directory, \"output_vectors.npy\")\n",
    "    output_feature_grid_file = os.path.join(output_directory, \"output_vector_feature_grid.npy\")\n",
    "    output_feature_mappings_file = os.path.join(output_directory, \"output_vector_feature_mappings.npy\")\n",
    "    output_geospatial_info_file = os.path.join(output_directory, \"output_vector_geospatial_info.npy\")\n",
    "\n",
    "    # Save the files\n",
    "    np.save(output_array_file, vector_data)\n",
    "    np.save(output_feature_grid_file, vector_feature_grids)\n",
    "    np.save(output_feature_mappings_file, all_feature_mappings)\n",
    "    np.save(output_geospatial_info_file, vector_geospatial_info_list)\n",
    "\n",
    "    print(f\"Files saved in: {output_directory}\")\n",
    "\n",
    "# Destroy the root window after file dialogs are closed\n",
    "root.destroy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PREEXISTING DATA ////////// IMPORT NETCDF AS ARRAY\n",
    "\n",
    "# Function to prompt the user to select a .nc file\n",
    "def select_nc_file():\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    root.attributes(\"-topmost\", True)  # Bring the dialog to the front\n",
    "\n",
    "    # Open a file selection dialog\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select NetCDF File\",\n",
    "        filetypes=[(\"NetCDF files\", \"*.nc\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "\n",
    "    root.destroy()  # Close the root window after file selection\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "# Function to convert a NetCDF file to a NumPy array and extract layer names\n",
    "def nc_to_numpy_and_layers(file_path):\n",
    "    try:\n",
    "        # Load the NetCDF file using xarray\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        \n",
    "        # Display available variables\n",
    "        print(\"Variables available in the NetCDF file:\")\n",
    "        for var in ds.data_vars:\n",
    "            print(var)\n",
    "        \n",
    "        # Select the variable to convert to NumPy array (adjust as needed)\n",
    "        var_name = input(\"Enter the variable name to convert to NumPy array: \")\n",
    "\n",
    "        if var_name in ds:\n",
    "            # Extract the variable and convert it to a NumPy array\n",
    "            data_array = ds[var_name].values\n",
    "            print(f\"Successfully converted {var_name} to a NumPy array with shape {data_array.shape}\")\n",
    "            \n",
    "            # Extract layer names (if the variable is multi-dimensional)\n",
    "            if 'layer' in ds[var_name].dims:\n",
    "                combined_layer_names = ds[var_name].coords['layer'].values.tolist()\n",
    "            else:\n",
    "                # If no specific layer dimension, use the variable name\n",
    "                combined_layer_names = [var_name]\n",
    "            \n",
    "            print(f\"Layer names: {combined_layer_names}\")\n",
    "            return data_array, combined_layer_names\n",
    "        else:\n",
    "            print(f\"Variable '{var_name}' not found in the NetCDF file.\")\n",
    "            return None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NetCDF file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 1: Select the NetCDF file\n",
    "selected_file = select_nc_file()\n",
    "\n",
    "if selected_file:\n",
    "    print(f\"Selected file: {selected_file}\")\n",
    "    \n",
    "    # Step 2: Convert the selected NetCDF file to a NumPy array and extract layer names\n",
    "    nc_array, nc_names = nc_to_numpy_and_layers(selected_file)\n",
    "    \n",
    "    if nc_array is not None:\n",
    "        print(f\"Converted NumPy array shape: {nc_array.shape}\")\n",
    "        print(f\"Combined Layer Names: {nc_names}\")\n",
    "else:\n",
    "    print(\"No file selected.\")\n",
    "\n",
    "combined_data = nc_array\n",
    "combined_layer_names = nc_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.5.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.5.0/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.5.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.5.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.5.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.5.2.min.js\", \"https://cdn.holoviz.org/panel/1.5.0/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='0588ff09-4e52-4def-a762-85b4e2d03435'>\n",
       "  <div id=\"e9dbdcc9-00df-4806-8266-f6612af15cf5\" data-root-id=\"0588ff09-4e52-4def-a762-85b4e2d03435\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"ba6eb297-8ddb-4a01-a5cd-fe732c29eaad\":{\"version\":\"3.5.2\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"0588ff09-4e52-4def-a762-85b4e2d03435\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"44d6ac6b-f767-4766-8603-32ccb4b98296\",\"attributes\":{\"plot_id\":\"0588ff09-4e52-4def-a762-85b4e2d03435\",\"comm_id\":\"2c0b8c8f0a0c40cab431ced16913be8e\",\"client_comm_id\":\"f08ebd6f10ac45f4bf366c49f6eb73ab\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\"},{\"type\":\"model\",\"name\":\"JSComponent1\"},{\"type\":\"model\",\"name\":\"ReactComponent1\"},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\"},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"ba6eb297-8ddb-4a01-a5cd-fe732c29eaad\",\"roots\":{\"0588ff09-4e52-4def-a762-85b4e2d03435\":\"e9dbdcc9-00df-4806-8266-f6612af15cf5\"},\"root_ids\":[\"0588ff09-4e52-4def-a762-85b4e2d03435\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "0588ff09-4e52-4def-a762-85b4e2d03435"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b58656b38044e27aef3741ef9b2b03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'522f0d6f-0627-4210-8b27-402cdf54dcee': {'version…"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PLOT COMBINED - with layer names\n",
    "\n",
    "data_to_plot = combined_data\n",
    "\n",
    "# Initialize the Panel extension\n",
    "pn.extension()\n",
    "\n",
    "# Assuming combined_layer_names is a list that stores the names of each layer\n",
    "# combined_layer_names should be the list that maps to each layer of the combined_data array\n",
    "\n",
    "# Function to plot a specific layer using Matplotlib\n",
    "def plot_layer_bokeh(layer_index):\n",
    "    # Create the plot\n",
    "    fig = Figure(figsize=(4, 3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(data_to_plot[layer_index], cmap='viridis', interpolation='nearest', aspect='auto')\n",
    "    \n",
    "    # Use combined_layer_names for the title\n",
    "    ax.set_title(f\"Layer {layer_index + 1}: {combined_layer_names[layer_index]}\")\n",
    "    \n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "    return pn.pane.Matplotlib(fig, tight=True)\n",
    "\n",
    "# Create a Bokeh slider widget for selecting the layer\n",
    "layer_slider = pn.widgets.IntSlider(name='Layer Index', start=0, end=data_to_plot.shape[0] - 1, step=1, value=0)\n",
    "\n",
    "# Bind the plotting function to the slider value\n",
    "panel = pn.bind(plot_layer_bokeh, layer_index=layer_slider)\n",
    "\n",
    "# Display the Panel with the slider and plot\n",
    "pn.Column(layer_slider, panel).servable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE SIMPLE PLOT TO EXPORT\n",
    "plt.figure(figsize=(10,14))\n",
    "plt.imshow(combined_data[0])\n",
    "plt.imshow(combined_data[1])\n",
    "plt.title(\"Buffered Targets Layer\", fontsize=15)\n",
    "plt.xlabel(\"X Coordinate\", fontsize=10)\n",
    "plt.ylabel(\"Y Coordinate\", fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TO XARRAY\n",
    "\n",
    "# Create dummy arrays for X, Y coordinates (you can replace these with your actual coordinates)\n",
    "x_coords = np.arange(combined_data.shape[2])  # X-coordinates (along the third axis)\n",
    "y_coords = np.arange(combined_data.shape[1])  # Y-coordinates (along the second axis)\n",
    "layer_names = combined_layer_names  # Layer names\n",
    "\n",
    "# Create an xarray DataArray from the combined NumPy array\n",
    "data_xr = xr.DataArray(\n",
    "    combined_data, \n",
    "    dims=[\"layer\", \"y\", \"x\"], \n",
    "    coords={\"layer\": layer_names, \"y\": y_coords, \"x\": x_coords},\n",
    "    name=\"combined_layers\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT DATA ////////// EXPORT XARRAY TO NETCDF\n",
    "\n",
    "# Hide the root window for the file dialog\n",
    "root = Tk()\n",
    "root.withdraw()  # Hide the main window\n",
    "root.attributes(\"-topmost\", True)  # Bring the file dialog to the front\n",
    "\n",
    "# Prompt the user to select a location to save the NetCDF file\n",
    "output_file = asksaveasfilename(\n",
    "    initialfile=\"THE_CUBE.nc\",  # Default file name\n",
    "    defaultextension=\".nc\",  # Default extension\n",
    "    filetypes=[(\"NetCDF files\", \"*.nc\"), (\"All files\", \"*.*\")],\n",
    "    title=\"Save NetCDF file\"\n",
    ")\n",
    "\n",
    "# If the user provides a location, save the NetCDF file\n",
    "if output_file:\n",
    "    data_xr.to_netcdf(output_file)\n",
    "    print(f\"Data successfully exported to {output_file}\")\n",
    "\n",
    "# Destroy the root window after file dialog is closed\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDIVIDUAL FUNCTIONS FOR TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNIFIED PROCESSING FUNCTION\n",
    "def process_file(file, file_type, grid_size, mask_file=None, vector_features_to_process=None):\n",
    "    if file_type == 'target':\n",
    "        # Buffer and rasterize targets\n",
    "        target_buffer_size = get_buffer_size()\n",
    "        target_df = gpd.read_file(file)\n",
    "        target_df_projected = target_df.to_crs(\"EPSG:3857\")\n",
    "        target_df_projected['geometry'] = target_df_projected.geometry.buffer(target_buffer_size)\n",
    "        bounds = target_df_projected.total_bounds\n",
    "        target_transform = from_bounds(bounds[0], bounds[1], bounds[2], bounds[3], grid_size[1], grid_size[0])\n",
    "        target_geometry_generator = ((geom, 1) for geom in target_df_projected.geometry)\n",
    "        target_data = rasterize(shapes=target_geometry_generator, out_shape=grid_size, fill=0, transform=target_transform).astype('float32')\n",
    "        target_data_3D = np.expand_dims(target_data, axis=0)\n",
    "        target_layer_name = f\"TARGET_{os.path.basename(file).replace('.geojson', '')}\"\n",
    "        return target_data_3D, target_layer_name\n",
    "\n",
    "    elif file_type == 'vector':\n",
    "        # Process vector data to create numpy grid\n",
    "        gdf = gpd.read_file(file)\n",
    "        gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "        feature_columns = [col for _, col in vector_features_to_process if _ == file]\n",
    "        vector_layers = []\n",
    "        vector_layer_names = []\n",
    "        for feature_column in feature_columns:\n",
    "            unique_categories = gdf[feature_column].unique()\n",
    "            category_to_int = {cat: i for i, cat in enumerate(unique_categories)}\n",
    "            sindex = gdf.sindex\n",
    "            x = np.linspace(gdf.total_bounds[0], gdf.total_bounds[2], grid_size[1] + 1)\n",
    "            y = np.linspace(gdf.total_bounds[1], gdf.total_bounds[3], grid_size[0] + 1)\n",
    "            cells = [box(x[j], y[i], x[j + 1], y[i + 1]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "            results = Parallel(n_jobs=-1)(delayed(process_cell)(idx, cell, gdf, sindex, feature_column, category_to_int, grid_size) for idx, cell in enumerate(cells))\n",
    "            grid = np.full(grid_size, np.nan)\n",
    "            for i, j, value in results:\n",
    "                grid[i, j] = value\n",
    "            grid_flipped = np.flipud(grid)\n",
    "            vector_layers.append(np.expand_dims(grid_flipped, axis=0))\n",
    "            vector_layer_names.append(f\"{os.path.basename(file).replace('.geojson', '')}_{feature_column}\")\n",
    "        return np.concatenate(vector_layers, axis=0), vector_layer_names\n",
    "\n",
    "    elif file_type == 'line':\n",
    "        # Process line vector files with buffering and distance calculation\n",
    "        buffer_lines = user_buffer_choice()\n",
    "        buffer_distance_meters = get_buffer_distance_meters() if buffer_lines else 0\n",
    "        gdf = gpd.read_file(file)\n",
    "\n",
    "        # Reproject to a projected CRS if necessary (e.g., UTM)\n",
    "        if not gdf.crs or not gdf.crs.is_projected:\n",
    "            print(\"Reprojecting to a UTM CRS for accurate distance buffering...\")\n",
    "            gdf = gdf.to_crs(gdf.estimate_utm_crs())\n",
    "\n",
    "        # Calculate pixel distance if buffering is enabled\n",
    "        pixel_distance = 0\n",
    "        if buffer_lines:\n",
    "            pixel_distance = calculate_pixel_distance(gdf, grid_size, buffer_distance_meters)\n",
    "            print(f\"Buffer distance in pixels: {pixel_distance}\")\n",
    "\n",
    "        # Create a binary grid of the line geometries\n",
    "        transform = rasterio.transform.from_bounds(*gdf.total_bounds, grid_size[1], grid_size[0])\n",
    "        shapes = []\n",
    "\n",
    "        # Iterate through each geometry, ensuring it is not None\n",
    "        for geom in gdf.geometry:\n",
    "            if geom is None:\n",
    "                continue  # Skip None geometries\n",
    "            if geom.geom_type == 'LineString':\n",
    "                shapes.append((geom, 1))\n",
    "            elif geom.geom_type == 'MultiLineString':\n",
    "                for line in geom.geoms:\n",
    "                    if isinstance(line, LineString):\n",
    "                        shapes.append((line, 1))\n",
    "\n",
    "        # Rasterize the geometries to create the binary grid\n",
    "        if shapes:\n",
    "            binary_grid = rasterize(shapes, out_shape=grid_size, transform=transform, fill=0, dtype='uint8')\n",
    "\n",
    "            if buffer_lines and pixel_distance > 0:\n",
    "                raster_map = calculate_distance(binary_grid, max_distance=pixel_distance)\n",
    "            else:\n",
    "                raster_map = binary_grid.astype(np.float32)\n",
    "        else:\n",
    "            print(f\"No valid line geometries found in {file}. Skipping...\")\n",
    "            raster_map = np.zeros(grid_size, dtype=np.float32)\n",
    "\n",
    "        line_layer_name = f\"LINE_{os.path.basename(file).replace('.geojson', '')}\"\n",
    "        return np.expand_dims(raster_map, axis=0), line_layer_name\n",
    "\n",
    "    elif file_type == 'raster':\n",
    "        # Process raster files\n",
    "        if not mask_file:\n",
    "            print(\"No mask file provided. Skipping raster processing.\")\n",
    "            return None, None\n",
    "        mask_gdf = gpd.read_file(mask_file)\n",
    "        minx, miny, maxx, maxy = mask_gdf.total_bounds\n",
    "        mask_gdf = mask_gdf.to_crs(\"EPSG:4326\")\n",
    "        minx, miny, maxx, maxy = mask_gdf.total_bounds\n",
    "        raster_target_transform = from_bounds(minx, miny, maxx, maxy, grid_size[1], grid_size[0])\n",
    "        raster_target_crs = \"EPSG:4326\"\n",
    "        with rasterio.open(file, 'r') as src:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            src_crs = src.crs if src.crs != raster_target_crs else raster_target_crs\n",
    "            raster_data_array = np.full(grid_size, np.nan, dtype=np.float32)\n",
    "            nodata_value = src.nodata if src.nodata is not None else np.nan\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=raster_data_array,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src_crs,\n",
    "                dst_transform=raster_target_transform,\n",
    "                dst_crs=raster_target_crs,\n",
    "                resampling=Resampling.nearest,\n",
    "                src_nodata=nodata_value,\n",
    "                dst_nodata=np.nan\n",
    "            )\n",
    "        raster_name = os.path.basename(file).replace('.tiff', '').replace('.tif', '')\n",
    "        return np.expand_dims(raster_data_array, axis=0), raster_name\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type provided.\")\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def get_buffer_size():\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes(\"-topmost\", True)\n",
    "    buffer_size = simpledialog.askinteger(\"Input\", \"Enter the target buffer size in meters:\", minvalue=1)\n",
    "    root.destroy()\n",
    "    return buffer_size\n",
    "\n",
    "def process_cell(idx, cell, gdf, sindex, feature_column, category_to_int, grid_size):\n",
    "    i, j = divmod(idx, grid_size[1])\n",
    "    possible_matches_index = list(sindex.intersection(cell.bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    intersections = possible_matches.geometry.intersection(cell)\n",
    "    valid_intersections = intersections[intersections.area > 0]\n",
    "    if valid_intersections.empty:\n",
    "        return i, j, np.nan\n",
    "    largest_intersection_idx = valid_intersections.area.idxmax()\n",
    "    category = possible_matches.loc[largest_intersection_idx, feature_column]\n",
    "    return i, j, category_to_int[category]\n",
    "\n",
    "# Function to calculate normalized distance\n",
    "def calculate_distance(arr, max_distance=20, dtype=np.float32):\n",
    "    arr = np.asarray(arr, dtype=bool)\n",
    "    dist = distance_transform_edt(~arr)\n",
    "    normalized_dist = np.clip(1 - dist / max_distance, 0, 1)\n",
    "    return normalized_dist.astype(dtype)\n",
    "\n",
    "# Function to calculate pixel distance from buffer distance in meters\n",
    "def calculate_pixel_distance(gdf, grid_size, buffer_distance_meters):\n",
    "    height, width = grid_size\n",
    "    bounds = gdf.total_bounds\n",
    "    x_range_meters = bounds[2] - bounds[0]\n",
    "    y_range_meters = bounds[3] - bounds[1]\n",
    "    x_res = x_range_meters / width\n",
    "    y_res = y_range_meters / height\n",
    "    pixel_distance = buffer_distance_meters / ((x_res + y_res) / 2)\n",
    "    return int(pixel_distance)\n",
    "\n",
    "# Function for user to choose between buffering or rasterizing\n",
    "def user_buffer_choice():\n",
    "    choice = {\"buffer\": None}\n",
    "\n",
    "    def set_choice_buffer():\n",
    "        choice[\"buffer\"] = True\n",
    "        window.destroy()\n",
    "\n",
    "    def set_choice_rasterize():\n",
    "        choice[\"buffer\"] = False\n",
    "        window.destroy()\n",
    "\n",
    "    window = Tk()\n",
    "    window.title(\"Choose Processing Option\")\n",
    "    label = Label(window, text=\"Would you like to buffer the lines (calculate distances) or just rasterize them?\")\n",
    "    label.pack(pady=10)\n",
    "    buffer_button = Button(window, text=\"Buffer (Calculate Distance)\", command=set_choice_buffer)\n",
    "    buffer_button.pack(side=\"left\", padx=20, pady=20)\n",
    "    rasterize_button = Button(window, text=\"Rasterize Only\", command=set_choice_rasterize)\n",
    "    rasterize_button.pack(side=\"right\", padx=20, pady=20)\n",
    "    window.mainloop()\n",
    "    return choice[\"buffer\"]\n",
    "\n",
    "# Function to get buffer distance from the user\n",
    "def get_buffer_distance_meters():\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    try:\n",
    "        buffer_distance = simpledialog.askfloat(\"Input Buffer Distance\", \"Please enter buffer distance in meters:\")\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Invalid Input\", \"Please enter a valid number.\")\n",
    "        buffer_distance = None\n",
    "    root.destroy()\n",
    "    return buffer_distance\n",
    "\n",
    "\n",
    "\n",
    "# Process targets\n",
    "target_data = []\n",
    "target_layer_names = []\n",
    "if target_files:\n",
    "    for file in target_files:\n",
    "        target_layer, target_layer_name = process_file(file, 'target', grid_size)\n",
    "        target_data.append(target_layer)\n",
    "        target_layer_names.append(target_layer_name)\n",
    "    target_data = np.concatenate(target_data, axis=0)\n",
    "    print(f\"Target data shape: {target_data.shape}\")\n",
    "else:\n",
    "    print(\"No target files detected.\")\n",
    "    target_data = np.array([])\n",
    "    target_layer_names = []\n",
    "\n",
    "# Process rasters\n",
    "raster_data = []\n",
    "raster_names = []\n",
    "if raster_files:\n",
    "    if not mask_file:\n",
    "        print(\"No mask file selected. Skipping raster processing.\")\n",
    "    else:\n",
    "        for file in raster_files:\n",
    "            raster_layer, raster_name = process_file(file, 'raster', grid_size, mask_file=mask_file)\n",
    "            if raster_layer is not None:\n",
    "                raster_data.append(raster_layer)\n",
    "                raster_names.append(raster_name)\n",
    "        if raster_data:\n",
    "            raster_data = np.concatenate(raster_data, axis=0)\n",
    "            print(f\"Raster data shape: {raster_data.shape}\")\n",
    "        else:\n",
    "            print(\"No raster data processed.\")\n",
    "else:\n",
    "    print(\"No raster files detected.\")\n",
    "    raster_data = np.array([])\n",
    "    raster_names = []\n",
    "\n",
    "# Process line vectors\n",
    "line_vector_data = []\n",
    "line_layer_names = []\n",
    "if line_geojson_files:\n",
    "    for file in line_geojson_files:\n",
    "        line_layer, line_layer_name = process_file(file, 'line', grid_size)\n",
    "        line_vector_data.append(line_layer)\n",
    "        line_layer_names.append(line_layer_name)\n",
    "    line_vector_data = np.concatenate(line_vector_data, axis=0)\n",
    "    print(f\"Line vector data shape: {line_vector_data.shape}\")\n",
    "else:\n",
    "    print(\"No line vector files selected.\")\n",
    "    line_vector_data = np.array([])\n",
    "    line_layer_names = []\n",
    "\n",
    "# Process polygon vectors\n",
    "vector_data = []\n",
    "vector_feature_grids = {}\n",
    "vector_layer_names = []\n",
    "if geojson_files and vector_features_to_process:\n",
    "    for file in geojson_files:\n",
    "        vector_layer, feature_names = process_file(file, 'vector', grid_size, vector_features_to_process=vector_features_to_process)\n",
    "        vector_data.append(vector_layer)\n",
    "        vector_layer_names.extend(feature_names)\n",
    "        for feature_name, layer in zip(feature_names, vector_layer):\n",
    "            vector_feature_grids[feature_name] = layer\n",
    "    vector_data = np.concatenate(vector_data, axis=0)\n",
    "    print(f\"Vector data shape: {vector_data.shape}\")\n",
    "else:\n",
    "    print(\"No polygon vector files or features selected.\")\n",
    "    vector_data = np.array([])\n",
    "    vector_feature_grids = {}\n",
    "    vector_layer_names = []\n",
    "\n",
    "\n",
    "\n",
    "print(f\"vector_data shape: {vector_data.shape}\")\n",
    "print(f\"line_vector_data shape: {line_vector_data.shape}\")\n",
    "print(f\"raster_data shape: {raster_data.shape}\")\n",
    "print(f\"target_data shape: {target_data.shape}\")\n",
    "\n",
    "combined_data = []\n",
    "combined_layer_names = []\n",
    "\n",
    "# Create a list to hold non-empty data arrays and their corresponding layer names\n",
    "data_arrays = []\n",
    "layer_names = []\n",
    "\n",
    "# Function to add non-empty data arrays and their names\n",
    "def add_data_and_names(data_array, names):\n",
    "    if data_array.size != 0:\n",
    "        data_arrays.append(data_array)\n",
    "        layer_names.extend(names)\n",
    "    else:\n",
    "        print(f\"Skipping empty data array with names: {names}\")\n",
    "\n",
    "# Add vector data\n",
    "if vector_data.size != 0:\n",
    "    add_data_and_names(vector_data, vector_layer_names)\n",
    "else:\n",
    "    print(\"Vector data is empty.\")\n",
    "\n",
    "# Add raster data\n",
    "if raster_data.size != 0:\n",
    "    add_data_and_names(raster_data, raster_names)\n",
    "else:\n",
    "    print(\"Raster data is empty.\")\n",
    "\n",
    "# Add line vector data\n",
    "if line_vector_data.size != 0:\n",
    "    add_data_and_names(line_vector_data, line_layer_names)\n",
    "else:\n",
    "    print(\"Line vector data is empty.\")\n",
    "\n",
    "# Add target data\n",
    "if target_data.size != 0:\n",
    "    add_data_and_names(target_data, target_layer_names)\n",
    "else:\n",
    "    print(\"Target data is empty.\")\n",
    "\n",
    "# Check if we have at least one non-empty data array\n",
    "if data_arrays:\n",
    "    # Check that all arrays have the same x and y dimensions\n",
    "    first_shape = data_arrays[0].shape[1:]  # Skip the first dimension (number of layers)\n",
    "    shapes_match = all(data_array.shape[1:] == first_shape for data_array in data_arrays)\n",
    "\n",
    "    if shapes_match:\n",
    "        # Concatenate all data arrays along axis=0\n",
    "        combined_data = np.concatenate(data_arrays, axis=0)\n",
    "        combined_layer_names = layer_names\n",
    "\n",
    "        print(f\"Combined array shape (with targets): {combined_data.shape}\")\n",
    "\n",
    "        # Check the mapping to ensure it is correct\n",
    "        print(\"Layer Name Mapping List:\", combined_layer_names)\n",
    "\n",
    "        # Ensure the combined_data layers match the number of names\n",
    "        if len(combined_layer_names) == combined_data.shape[0]:\n",
    "            print(f\"Layer name mapping successful. Total layers: {len(combined_layer_names)}\")\n",
    "        else:\n",
    "            print(f\"Warning: Mismatch in layers. {len(combined_layer_names)} names for {combined_data.shape[0]} layers.\")\n",
    "    else:\n",
    "        print(\"Error: The x/y dimensions of the arrays do not match.\")\n",
    "        print(\"Array shapes:\")\n",
    "        for idx, data_array in enumerate(data_arrays):\n",
    "            print(f\"Array {idx} shape: {data_array.shape}\")\n",
    "else:\n",
    "    print(\"No data arrays to combine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUFFER/RASTERIZE TARGETS\n",
    "\n",
    "# Function to ask user for buffer size\n",
    "def get_buffer_size():\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    root.attributes(\"-topmost\", True)  # Bring the dialog to the front\n",
    "    buffer_size = askinteger(\"Input\", \"Enter the buffer size in meters:\", minvalue=1)\n",
    "    root.destroy()  # Close the Tkinter root window\n",
    "    return buffer_size\n",
    "\n",
    "# Function to ask the user if they want to merge the files with Yes/No buttons\n",
    "def ask_merge_or_separate():\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    root.attributes(\"-topmost\", True)  # Bring the dialog to the front\n",
    "    response = messagebox.askyesno(\"Merge Targets\", \"Would you like to merge the target files?\")\n",
    "    root.destroy()\n",
    "    return response  # Returns True if 'Yes' is clicked, else False\n",
    "\n",
    "# Function to buffer and rasterize a single target file\n",
    "def buffer_and_rasterize(target_file, target_buffer_size, grid_size):\n",
    "    # Read the GeoJSON file\n",
    "    target_df = gpd.read_file(target_file)\n",
    "    \n",
    "    # Reproject to a CRS that allows buffering (e.g., UTM)\n",
    "    target_df_projected = target_df.to_crs(\"EPSG:3857\")  # UTM or other projected CRS for buffering\n",
    "    \n",
    "    # Buffer the targetnts by the specified buffer size (in meters)\n",
    "    target_df_projected['geometry'] = target_df_projected.geometry.buffer(target_buffer_size)\n",
    "    \n",
    "    # Create the transform using the bounds and grid size\n",
    "    bounds = target_df_projected.total_bounds\n",
    "    target_transform = from_bounds(bounds[0], bounds[1], bounds[2], bounds[3], grid_size[1], grid_size[0])\n",
    "\n",
    "    # Rasterize the buffered targetnts, setting the value of targetnts to 1, others to 0\n",
    "    target_geometry_generator = ((geom, 1) for geom in target_df_projected.geometry)\n",
    "    target_data = rasterize(shapes=target_geometry_generator, out_shape=grid_size, fill=0, transform=target_transform).astype('float32')\n",
    "\n",
    "    # Reshape the data into a 3D NumPy array with one layer for the data\n",
    "    target_data_3D = np.expand_dims(target_data, axis=0)  # Expands the data to have a layer dimension at the front\n",
    "    \n",
    "    # Return the 3D array\n",
    "    return target_data_3D\n",
    "\n",
    "\n",
    "def process_multiple_targets(grid_size, target_files):\n",
    "    # Initialize an empty list to store the combined target data\n",
    "    combined_target_data = []\n",
    "    \n",
    "    # Initialize a list to store the target layer names\n",
    "    target_layer_names = []\n",
    "    \n",
    "    # Ask the user if they want to merge the files or keep them separate\n",
    "    merge_targets = ask_merge_or_separate()\n",
    "    \n",
    "    if merge_targets:\n",
    "        # If merging, initialize an empty array for merged data\n",
    "        merged_target_data = np.zeros(grid_size)\n",
    "        for target_file in target_files:\n",
    "            # Ask for buffer size for each target file\n",
    "            target_buffer_size = get_buffer_size()\n",
    "            \n",
    "            # Buffer and rasterize the target file\n",
    "            target_data = buffer_and_rasterize(target_file, target_buffer_size, grid_size)\n",
    "            \n",
    "            # Add the rasterized data to the merged target data (values of 1 take precedence)\n",
    "            merged_target_data = np.maximum(merged_target_data, target_data[0])  # Use max to keep targetnts where there's overlap\n",
    "        \n",
    "        # After merging, expand the merged data to have a layer dimension and append to the combined list\n",
    "        combined_target_data.append(np.expand_dims(merged_target_data, axis=0))\n",
    "        target_layer_names.append(\"TARGETS\")  # Single layer name for merged data\n",
    "    else:\n",
    "        # If not merging, process each file separately\n",
    "        for idx, target_file in enumerate(target_files, start=1):\n",
    "            # Ask for buffer size for each target file\n",
    "            target_buffer_size = get_buffer_size()\n",
    "            \n",
    "            # Buffer and rasterize the target file\n",
    "            target_data = buffer_and_rasterize(target_file, target_buffer_size, grid_size)\n",
    "            \n",
    "            # Append the target data to the combined list\n",
    "            combined_target_data.append(target_data)\n",
    "            \n",
    "            # Get the filename without extension and use it in the layer name\n",
    "            target_filename = os.path.splitext(os.path.basename(target_file))[0]\n",
    "            target_layer_names.append(f\"TARGET_{target_filename}\")\n",
    "    \n",
    "    # Concatenate the list of target layers along the first axis (layer axis)\n",
    "    combined_target_data_3D = np.concatenate(combined_target_data, axis=0)\n",
    "    \n",
    "    # Return the combined data and layer names\n",
    "    return combined_target_data_3D, target_layer_names\n",
    "\n",
    "\n",
    "target_data, target_layer_names = process_multiple_targets(grid_size, target_files)\n",
    "\n",
    "# Check the shape of the combined result\n",
    "print(f\"Combined target_data shape: {target_data.shape}\")\n",
    "print(f\"Target layer names: {target_layer_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POLYGON VECTOR PROCESSING FUNCTIONS\n",
    "\n",
    "def process_cell(idx, cell, gdf, sindex, feature_column, category_to_int):\n",
    "    i, j = divmod(idx, grid_size[1])\n",
    "\n",
    "    # Use the spatial index to find potential intersecting polygons\n",
    "    possible_matches_index = list(sindex.intersection(cell.bounds))\n",
    "    if not possible_matches_index:\n",
    "        return i, j, np.nan  # Return NaN if no polygon is found\n",
    "\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    if possible_matches.empty:\n",
    "        return i, j, np.nan  # Return NaN if no polygon is found\n",
    "\n",
    "    # For polygons, find intersections\n",
    "    intersections = possible_matches.geometry.intersection(cell)\n",
    "    valid_intersections = intersections[intersections.area > 0]\n",
    "\n",
    "    if valid_intersections.empty:\n",
    "        return i, j, np.nan\n",
    "\n",
    "    if len(valid_intersections) > 5:\n",
    "        areas_per_category = {}\n",
    "        for idx, intersection in enumerate(valid_intersections):\n",
    "            if not intersection.is_empty:\n",
    "                category = possible_matches.iloc[idx][feature_column]\n",
    "                # Use the raw category (no file name prefix)\n",
    "                if category not in areas_per_category:\n",
    "                    areas_per_category[category] = 0\n",
    "                areas_per_category[category] += intersection.area\n",
    "\n",
    "        if areas_per_category:\n",
    "            max_category = max(areas_per_category, key=areas_per_category.get)\n",
    "            return i, j, category_to_int[max_category]\n",
    "        else:\n",
    "            return i, j, np.nan\n",
    "    else:\n",
    "        largest_intersection_idx = valid_intersections.area.idxmax()\n",
    "        category = possible_matches.loc[largest_intersection_idx, feature_column]\n",
    "        # Use raw category here as well\n",
    "        return i, j, category_to_int[category]\n",
    "\n",
    "\n",
    "# Function to process each feature column\n",
    "def process_feature_column(geojson_file, feature_column, grid_size, target_crs, x, y):\n",
    "    gdf = gpd.read_file(geojson_file)\n",
    "    print(f\"Processing feature column: {feature_column} from file: {geojson_file}\")\n",
    "\n",
    "    gdf = gdf.to_crs(target_crs)\n",
    "    if gdf.empty:\n",
    "        print(f\"GeoDataFrame for {geojson_file} is empty after reprojecting. Skipping column: {feature_column}\")\n",
    "        return None\n",
    "\n",
    "    # Remove filename from category_to_int key and only use field names for values\n",
    "    unique_categories = gdf[feature_column].unique()\n",
    "    category_to_int = {cat: i for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "    grid = np.full(grid_size, np.nan)  # Create an empty grid filled with NaN\n",
    "    sindex = gdf.sindex\n",
    "\n",
    "    cells = [box(x[j], y[i], x[j + 1], y[i + 1])\n",
    "             for i in range(grid_size[0])\n",
    "             for j in range(grid_size[1])]\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_cell)(\n",
    "        idx, cell, gdf, sindex, feature_column, category_to_int\n",
    "    ) for idx, cell in enumerate(cells))\n",
    "\n",
    "    if not results:\n",
    "        print(f\"No results were generated for feature column: {feature_column} from file: {geojson_file}\")\n",
    "        return None\n",
    "\n",
    "    for i, j, value in results:\n",
    "        grid[i, j] = value\n",
    "\n",
    "    # Flip the grid vertically (along Y-axis)\n",
    "    grid_flipped = np.flipud(grid)\n",
    "\n",
    "    # Strip '.geojson' from the file name to clean up field names\n",
    "    filename_cleaned = os.path.basename(geojson_file).replace('.geojson', '')\n",
    "    return (f\"{filename_cleaned}_{feature_column}\", grid_flipped, category_to_int)\n",
    "\n",
    "\n",
    "# Batch processing function\n",
    "def geojson_to_numpy_grid_3d_batch(\n",
    "    grid_size: Tuple[int, int],  # Grid size for the output array\n",
    "    geojson_files: List[str],  # List of GeoJSON files\n",
    "    features_to_process: List[Tuple[str, str]],  # List of (file, feature) tuples to process\n",
    "    target_crs: str = \"EPSG:3857\"  # Web Mercator projection\n",
    ") -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, Dict[Any, int]], List[Dict[str, Any]]]:\n",
    "    all_feature_grids = {}\n",
    "    all_feature_mappings = {}  # Initialize all_feature_mappings to track the category mappings\n",
    "    geospatial_info_list = []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Process each file and its corresponding features\n",
    "    for geojson_file in geojson_files:\n",
    "        # Get the filename without extension for prefixing the field names\n",
    "        filename_prefix = os.path.basename(geojson_file).replace('.geojson', '').replace('.json', '')\n",
    "\n",
    "        # Read the GeoJSON file to get the total bounds\n",
    "        gdf = gpd.read_file(geojson_file)\n",
    "        gdf = gdf.to_crs(target_crs)\n",
    "        minx, miny, maxx, maxy = gdf.total_bounds\n",
    "\n",
    "        x = np.linspace(minx, maxx, grid_size[1] + 1)\n",
    "        y = np.linspace(miny, maxy, grid_size[0] + 1)\n",
    "\n",
    "        # Extract relevant features for this file\n",
    "        file_features = [feature for file, feature in features_to_process if file == geojson_file]\n",
    "\n",
    "        # Store geospatial information for each file\n",
    "        geospatial_info = {\n",
    "            'transform': (minx, miny, maxx, maxy),\n",
    "            'crs': target_crs,\n",
    "            'file_name': filename_prefix\n",
    "        }\n",
    "        geospatial_info_list.append(geospatial_info)\n",
    "\n",
    "        # Use joblib to parallelize the processing of each feature column\n",
    "        results.extend(Parallel(n_jobs=-1)(delayed(process_feature_column)(\n",
    "            geojson_file, feature_column, grid_size, target_crs, x, y\n",
    "        ) for feature_column in file_features))\n",
    "\n",
    "    for feature_name, grid, category_to_int in results:\n",
    "        all_feature_grids[feature_name] = grid\n",
    "        all_feature_mappings[feature_name] = category_to_int  # Store category mappings for each feature\n",
    "\n",
    "    grid_3d = np.stack(list(all_feature_grids.values()), axis=0)\n",
    "\n",
    "    return grid_3d, all_feature_grids, all_feature_mappings, geospatial_info_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINE VECTOR PROCESSING FUNCTIONS\n",
    "\n",
    "# Define function to calculate distances\n",
    "def calculate_distance(arr, max_distance=20, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Calculate normalized distance from 1's in the input array.\n",
    "\n",
    "    Args:\n",
    "    arr (np.ndarray): Input binary array.\n",
    "    max_distance (float): Maximum distance to consider (default: 20).\n",
    "    dtype (np.dtype): Data type for the output array (default: np.float32).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Normalized distance array.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr, dtype=bool)\n",
    "\n",
    "    # Calculate the distance transform\n",
    "    dist = distance_transform_edt(~arr)\n",
    "\n",
    "    # Normalize the distance\n",
    "    normalized_dist = np.clip(1 - dist / max_distance, 0, 1)\n",
    "\n",
    "    return normalized_dist.astype(dtype)\n",
    "\n",
    "# Function to calculate the number of pixels corresponding to the distance in meters\n",
    "def calculate_pixel_distance(gdf, grid_size, buffer_distance_meters):\n",
    "    \"\"\"\n",
    "    Calculate the number of pixels corresponding to a given buffer distance in meters.\n",
    "\n",
    "    Args:\n",
    "    gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "    grid_size (tuple): The size of the output grid (height, width).\n",
    "    buffer_distance_meters (float): Buffer distance in meters.\n",
    "\n",
    "    Returns:\n",
    "    int: The buffer distance in pixels.\n",
    "    \"\"\"\n",
    "    height, width = grid_size\n",
    "\n",
    "    # Get the total bounds of the geometry\n",
    "    bounds = gdf.total_bounds  # (minx, miny, maxx, maxy)\n",
    "\n",
    "    # Calculate the width and height in meters (assuming CRS is projected)\n",
    "    x_range_meters = bounds[2] - bounds[0]  # maxx - minx\n",
    "    y_range_meters = bounds[3] - bounds[1]  # maxy - miny\n",
    "\n",
    "    # Calculate resolution (meters per pixel)\n",
    "    x_res = x_range_meters / width\n",
    "    y_res = y_range_meters / height\n",
    "\n",
    "    # Use the average resolution to convert meters to pixels\n",
    "    pixel_distance = buffer_distance_meters / ((x_res + y_res) / 2)\n",
    "\n",
    "    return int(pixel_distance)\n",
    "\n",
    "# Function to either buffer and calculate distance, or just rasterize\n",
    "def create_gridded_distance_map(geojson_files, grid_size, max_distance=20, buffer_lines=True, buffer_distance_meters=0):\n",
    "    \"\"\"\n",
    "    Generate a 3D NumPy array with either rasterized lines or distance maps.\n",
    "\n",
    "    Args:\n",
    "    geojson_files (list): List of file paths to GeoJSON files.\n",
    "    grid_size (tuple): The size of the output grid (height, width).\n",
    "    max_distance (float): Maximum distance for normalization.\n",
    "    buffer_lines (bool): If True, buffer the lines and calculate distances.\n",
    "    buffer_distance_meters (float): Buffer distance in meters.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: 3D NumPy array with either distance maps or rasterized lines.\n",
    "    \"\"\"\n",
    "    height, width = grid_size\n",
    "    num_files = len(geojson_files)\n",
    "\n",
    "    # Create an empty 3D array to store the layers\n",
    "    raster_map = np.zeros((num_files, height, width), dtype=np.float32)\n",
    "\n",
    "    for idx, geojson_file in enumerate(geojson_files):\n",
    "        print(f\"Processing {geojson_file}...\")\n",
    "\n",
    "        # Load the GeoJSON file\n",
    "        gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "        # Reproject to a projected CRS if necessary (e.g., UTM)\n",
    "        if not gdf.crs.is_projected:\n",
    "            print(\"Reprojecting to a UTM CRS for accurate distance buffering...\")\n",
    "            gdf = gdf.to_crs(gdf.estimate_utm_crs())\n",
    "\n",
    "        # If buffering is enabled, calculate pixel distance from meters\n",
    "        pixel_distance = 0\n",
    "        if buffer_lines:\n",
    "            pixel_distance = calculate_pixel_distance(gdf, grid_size, buffer_distance_meters)\n",
    "            print(f\"Buffer distance in pixels: {pixel_distance}\")\n",
    "\n",
    "        # Create a binary grid of the line geometries\n",
    "        transform = rasterio.transform.from_bounds(*gdf.total_bounds, width, height)\n",
    "        shapes = []\n",
    "\n",
    "        # Iterate through each geometry, ensuring it is not None\n",
    "        for geom in gdf.geometry:\n",
    "            if geom is None:\n",
    "                continue  # Skip None geometries\n",
    "            if geom.geom_type == 'LineString':\n",
    "                shapes.append((geom, 1))\n",
    "            elif geom.geom_type == 'MultiLineString':\n",
    "                # Use .geoms to iterate over individual LineString components\n",
    "                for line in geom.geoms:\n",
    "                    if isinstance(line, LineString):\n",
    "                        shapes.append((line, 1))\n",
    "\n",
    "        # Rasterize the geometries to create the binary grid (dtype='uint8')\n",
    "        if shapes:\n",
    "            binary_grid = rasterize(shapes, out_shape=(height, width), transform=transform, fill=0, dtype='uint8')\n",
    "\n",
    "            if buffer_lines and pixel_distance > 0:\n",
    "                # Only calculate the distance map if buffering is chosen and pixel distance is valid\n",
    "                raster_map[idx] = calculate_distance(binary_grid, max_distance=pixel_distance)\n",
    "            else:\n",
    "                # Just store the binary raster without distance calculation\n",
    "                raster_map[idx] = binary_grid\n",
    "        else:\n",
    "            print(f\"No valid line geometries found in {geojson_file}. Skipping...\")\n",
    "\n",
    "    return raster_map\n",
    "\n",
    "\n",
    "# Function to display a popup window with \"Rasterize\" and \"Buffer\" buttons\n",
    "def user_buffer_choice():\n",
    "    # Initialize a variable to store the user's choice\n",
    "    choice = {\"buffer\": None}\n",
    "\n",
    "    # Function to set the choice and close the window\n",
    "    def set_choice_buffer():\n",
    "        choice[\"buffer\"] = True\n",
    "        window.destroy()\n",
    "\n",
    "    def set_choice_rasterize():\n",
    "        choice[\"buffer\"] = False\n",
    "        window.destroy()\n",
    "\n",
    "    # Create a new window\n",
    "    window = Tk()\n",
    "    window.title(\"Choose Processing Option\")\n",
    "\n",
    "    # Create and place a label\n",
    "    label = Label(window, text=\"Would you like to buffer the lines (calculate distances) or just rasterize them?\")\n",
    "    label.pack(pady=10)\n",
    "\n",
    "    # Create buttons for the two options\n",
    "    buffer_button = Button(window, text=\"Buffer (Calculate Distance)\", command=set_choice_buffer)\n",
    "    buffer_button.pack(side=\"left\", padx=20, pady=20)\n",
    "\n",
    "    rasterize_button = Button(window, text=\"Rasterize Only\", command=set_choice_rasterize)\n",
    "    rasterize_button.pack(side=\"right\", padx=20, pady=20)\n",
    "\n",
    "    # Run the window's event loop\n",
    "    window.mainloop()\n",
    "\n",
    "    return choice[\"buffer\"]  # Return the user's choice\n",
    "\n",
    "# Function to ask the user for buffer distance in meters\n",
    "def get_buffer_distance_meters():\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    try:\n",
    "        # Ask for buffer distance in meters\n",
    "        buffer_distance = simpledialog.askfloat(\"Input Buffer Distance\", \"Please enter buffer distance in meters:\")\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Invalid Input\", \"Please enter a valid number.\")\n",
    "        buffer_distance = None\n",
    "    root.destroy()\n",
    "    return buffer_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS RASTERS\n",
    "\n",
    "#to defined grid with feature mappings\n",
    "\n",
    "# Get the bounding box of the mask file to use for target transform\n",
    "raster_gdf = gpd.read_file(mask_file)\n",
    "minx, miny, maxx, maxy = raster_gdf.total_bounds\n",
    "\n",
    "# Convert the GeoDataFrame to a projected CRS\n",
    "raster_gdf = raster_gdf.to_crs(\"EPSG:4326\")  # Project to a common projected CRS, e.g., EPSG:3857\n",
    "\n",
    "# Recalculate bounds in the projected CRS\n",
    "minx, miny, maxx, maxy = raster_gdf.total_bounds\n",
    "\n",
    "# Compute the target transform for the projected CRS\n",
    "raster_target_transform = from_bounds(minx, miny, maxx, maxy, grid_size[1], grid_size[0])\n",
    "raster_target_crs = \"EPSG:4326\"  # Set the target CRS to a projected coordinate system\n",
    "\n",
    "\n",
    "# Lists to store data and corresponding file names\n",
    "raster_data = []\n",
    "raster_names = []\n",
    "raster_feature_mappings = []  # List to store the mappings from filenames to their corresponding layers\n",
    "\n",
    "# Now, reproject and resample each raster to the common grid\n",
    "for layer_index, raster_file in enumerate(raster_files):\n",
    "    with rasterio.open(raster_file, 'r') as src:\n",
    "        print(f\"Processing file: {raster_file}\")\n",
    "        print(f\"Source CRS: {src.crs}\")\n",
    "        print(f\"Source Transform: {src.transform}\")\n",
    "        print(f\"Source Bounds: {src.bounds}\")\n",
    "\n",
    "        # Check if the source CRS matches the target CRS; reproject if needed\n",
    "        if src.crs != raster_target_crs:\n",
    "            src_crs = src.crs\n",
    "        else:\n",
    "            src_crs = raster_target_crs  # Keep the same CRS if already matching\n",
    "        \n",
    "        # Prepare the output array with NaN (representing no data)\n",
    "        raster_data_array = np.full(grid_size, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Handle NoData value\n",
    "        nodata_value = src.nodata\n",
    "        if nodata_value is None:\n",
    "            nodata_value = np.nan  # If NoData is not set, assume NaN\n",
    "\n",
    "        # Print debug information\n",
    "        print(f\"Reprojecting {raster_file} to target grid...\")\n",
    "        print(f\"Target CRS: {raster_target_crs}\")\n",
    "        print(f\"Target Transform: {raster_target_transform}\")\n",
    "        print(f\"Target Grid Size: {grid_size}\")\n",
    "\n",
    "        # Reproject the source data to the target grid\n",
    "        reproject(\n",
    "            source=rasterio.band(src, 1),\n",
    "            destination=raster_data_array,\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src_crs,\n",
    "            dst_transform=raster_target_transform,\n",
    "            dst_crs=raster_target_crs,\n",
    "            resampling=Resampling.nearest,\n",
    "            src_nodata=nodata_value,\n",
    "            dst_nodata=np.nan  # Use NaN as the destination NoData\n",
    "        )\n",
    "\n",
    "        # Append the resampled data and names to lists\n",
    "        raster_data.append(raster_data_array)  # Append the resampled data array\n",
    "        file_name = os.path.basename(raster_file).replace('.tiff', '').replace('.tif', '')\n",
    "        raster_names.append(file_name)\n",
    "\n",
    "        # Append the filename to feature mappings with its corresponding layer index\n",
    "        raster_feature_mappings.append((file_name, layer_index))\n",
    "\n",
    "# Stack list into a 3D numpy array\n",
    "raster_data = np.stack(raster_data, axis=0)  # Stack the list of arrays into a 3D numpy array\n",
    "print(raster_data.shape, raster_names)\n",
    "print(\"Feature Mappings:\", raster_feature_mappings)  # Print the feature mappings\n",
    "\n",
    "# Check if all data layers contain NaN\n",
    "for i in range(raster_data.shape[0]):\n",
    "    print(f\"Layer {i} ({raster_names[i]}):\")\n",
    "    if np.all(np.isnan(raster_data[i])):\n",
    "        print(\"  All values are NaN.\")\n",
    "    else:\n",
    "        print(f\"  Min value: {np.nanmin(raster_data[i])}\")\n",
    "        print(f\"  Max value: {np.nanmax(raster_data[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS LINE VECTORS\n",
    "\n",
    "if line_geojson_files:\n",
    "    # Popup window for the user to select between buffering or rasterizing\n",
    "    buffer_lines = user_buffer_choice()\n",
    "\n",
    "    buffer_distance_meters = 0  # Default no buffer\n",
    "\n",
    "    if buffer_lines:\n",
    "        # Ask the user for buffer distance if they chose to buffer the lines\n",
    "        buffer_distance_meters = get_buffer_distance_meters()\n",
    "        if buffer_distance_meters is None:\n",
    "            print(\"Buffer distance input was invalid or canceled. Skipping buffering.\")\n",
    "            buffer_lines = False\n",
    "\n",
    "    # Process files based on user's choice and buffer distance in meters\n",
    "    line_vector_data = create_gridded_distance_map(line_geojson_files, grid_size, buffer_distance_meters=buffer_distance_meters)\n",
    "\n",
    "    # Now, `raster_map` contains the 3D array with either distance maps or rasterized lines\n",
    "    print(\"Generated map shape:\", line_vector_data.shape)\n",
    "\n",
    "else:\n",
    "    print(\"No files selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS POLY VECTORS\n",
    "\n",
    "\n",
    "vector_data, vector_feature_grids, all_feature_mappings, vector_geospatial_info_list = geojson_to_numpy_grid_3d_batch(\n",
    "    grid_size, geojson_files, vector_features_to_process\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Shape of the 3D grid array:\", vector_data.shape)\n",
    "print(\"Feature grids:\", vector_feature_grids.keys())\n",
    "print(\"Feature mappings:\", all_feature_mappings)\n",
    "print(\"Geospatial information for each file:\", vector_geospatial_info_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
